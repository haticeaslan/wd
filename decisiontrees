Decision Trees
Tree-based yani ağaç tabanlı yöntemler yorumlama için basit ve kullanışlı yöntemlerdendir. Verilen belirli bir gözlem için tahmin yapmak 
amacıyla genellikle eğitim gözlemlerinin ortalaması ya da ait olduğu bölgedeki modu kullanılır. Tahmini alanı bölmek için kullanılan bölme
kuralları kümesi bir ağaçta özetlenebilir ve bu tip yaklaşımlar karar ağaçları olarak adlandırılır. Karar ağaçları hem regresyon hem de
sınıflandırma problemlerine uygulanabilir. Birden fazla bağımsız parametre ve bir bağıml parametre içeren data setinde, bağımlı parametre
kategorik ise sınıflandırma ağacı, sürekli yapıda ise regresyon ağacı adını alır.
Amaç hem kategorik hem nümerik verilerden faydalanarak çıkarılan basit karar kuralları ile bir hedef değişkeninin değerini 
öngören model oluşturmaktır. 
Karar ağaçları karar düğümleri ve yaprak düğümlerinden oluşur.
Veri madenciliğinde iki tip karar ağacı kullanılır: 
Sınıflandırma ağacı analizi ve regresyon analizi
( https://en.wikipedia.org/wiki/Decision_tree )
Sınıflandırma ağacı girdi olarak iki dizi alır: alıştırma örnekleri tutan seyrek veya yoğun bir dizi ve boyutu [n_samples, n_features] 
olan X dizisi, alıştırma örnekleri için sınıf etiketlerini tutan ve tamsayı değerler alan Y boyut [n_samples] dizisi.
>>> from sklearn import tree
>>> X = [[0, 0], [1, 1]]
>>> Y = [0, 1]
>>> clf = tree.DecisionTreeClassifier()
>>> clf = clf.fit(X, Y)
>>> clf.predict([[2., 2.]])
array([1])
( http://scikit-learn.org/stable/modules/tree.html )
Veriyi genelleştirmeyen aşırı karmaşık ağaçlar oluşması, verideki küçük bir değişikliğin tamamen farklı bir yapı oluşturması,
öğrenmesi zor kavramlar içermesi gibi nedenler karar ağaçlarının dezavantajlarındandır.
En ayırt edici veriyi belirlemek için bilgi kazancı ölçülür. Bilgi kazancı ölçümünde entropi kullanılır.
Satır sınıflandırmak için gerekli bilgi :  I(s1,s2,...,sm)=-∑i=1,m (si/s)log2(si/s)
Entropi=E(A): ∑[(s1j+...+smj)/s]I(s1j,...,smj)
Kazanç(A)=I(s1,...,sm)-E(A)
(www.ayhandemiriz.com/SakaryaWebSite/slides/DMHafta5.ppt)
