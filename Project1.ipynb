{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Destek Vektör Makinesi ve Karar Ağaçları Algoritmalarının Veri Seti Üzerine Uygulanması\n",
    "\n",
    "## Üyeler\n",
    "\n",
    "1. Birinci üye: Berna Soysal soysalb@itu.edu.tr\n",
    "2. İkinci üye: Hatice Aslan aslanhat@itu.edu.tr\n",
    "\n",
    "## Projenin tanımı\n",
    "\n",
    "Bu projede Cam Teşhisi veri seti üzerinde destek vektör makinaları(SVM) ve karar ağaçları(Decision Tree) kullanılarak veri setinin analizinin yapılması amaçlanmaktadır.Python 3.6.0 ile Scikit-learn kütüphaneleri(library) kullanılarak sonuç alınacaktır. Veri setimizde bulunan nitelikler(attribute) her iki algoritmada sonuca ulaşmak için oldukça önemlidir. Veri setimiz suç mahallerinden kanıt için toplanmış olup camların içerdiği elementlerin çeşitleri ve ağırlık yüzdeleri bilgilerini içermektedir.SVM ve karar ağaçları algoritmaları ile beraber camlara ait bu bilgiler kullanılarak camların hangi cisimlerin yapımında kullanıldığının analizi yapılacaktır. Daha sonra bu iki algoritma karşılaştırılarak hangi algoritmanın daha iyi sonuç aldığı gözlemlenecektir.\n",
    "\n",
    "\n",
    "#### Destek Vektör Makinalar(SVM)\n",
    "SVM (Support Vector Machines-Destek Vektör Makineleri) verileri sınıflandırma ve regresyon analizinde kullanılan, öngörülen ve gerçek değer arasındaki farkı en aza indirmeyi hedefleyen denetlenen öğrenme modellerinden biridir.[Kaynak 1](Girosi, 1998)R.A. Fisher  N-boyutlu uzayda her bir veri her özelliği bir değere sahip olacak şekilde bir koordinat ile belirlenir.Sınıflandırma için sınıfları ayıracak bir hiperdüzlem veya hiperdüzlem kümesi bulunur.[Kaynak 2](http://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation) Bulunan hiperdüzlem sayesinde verilerin tamamı lineer bağımsız veya bir çoğu lineer bağımsız hale gelir. SVM iki farklı sınıf arasındaki bu sınırın nasıl çizilmesi gerektiğini verilen bağımlı ve bağımsız değişkenler arasındaki ilişkiyi öngörmek için kullanılan veri olarak tanımlayabileceğimiz eğitim seti üzerinden eğitim algoritması yoluyla belirler.  \n",
    "\n",
    "[Kaynak 3](https://en.wikipedia.org/wiki/Support_vector_machine#Parameter_selection) ve  \n",
    "[Kaynak 4](https://www.analyticsvidhya.com/blog/2015/10/understaing-support-vector-machine-example-code/)\n",
    "\n",
    "\n",
    "SVM bir çok problemi başarıyla çözebilir:\n",
    "+ Text ve hypertext kategorizasyonu\n",
    "+ Email filtreleme\n",
    "+ Belgeleri konusuna göre sınıflandırma\n",
    "+ Web aramaları\n",
    "+ İmage sınıflandırması\n",
    "+ Protein sınıflandırması\n",
    "+ Kanser sınıflandırması\n",
    "+ El yazısı tanımlama\n",
    "bunlardan bazılarıdır.[Kaynak 5](www.cs.utexas.edu/users/mooney/cs391L/svm.ppt) \n",
    "\n",
    "Eğitim örneği formunda\n",
    "\n",
    " + $\\{x_i, y_i\\}$, $i=1,...,n$ ve $x_i \\in \\mathbb{R}^d$ ve $y_i \\in \\{−1, +1\\}$\n",
    " + $\\{x_i\\}$: eşdeğişkenler veya girdi vektörü\n",
    " + $\\{y_i\\}$: bağımlı değişkenler\n",
    " olarak tanımlanır.\n",
    "\n",
    "\n",
    "Doğrusal ayrılabilen bir veri setini düşünürsek $\\ f(x)=w^T x_i − b $ olacak şekilde düz bir çizgi ile verileri sınıflandırabiliriz. Rastgele seçilen bir verinin  $\\ y_i=-1 $ değerini alması ikiye ayrılan bölgenin hangi tarafında olduğunu gösterir ve bu durumda $\\ f(x_i)<0 $ olur. Diğer taraftan $\\ y_i=1 $ değeri ise verinin diğer bölgede yer aldığını ve $\\ f(x_i)>0 $ olduğunu gösterir.Böylelikle elde edeceğimiz test durumlarını $\\ y_{test}=sign(x_{test}) $ kuralına göre sınıflandırabiliriz. \n",
    "\n",
    "Bu yol ile çizilebilecek hiperdüzlemlerin sayısı sonsuzdur. Seçilen hiperdüzlemin verdiği sonuçlar test durumlarında farklı performans gösterebilir. Belirli bir gruba yakın seçilecek hiperdüzlem diğer grup elemanlarının sınıflandırılmasında hataya neden olur. Bu neden seçilecek hiperdüzlem iki sınıfın eğitim setlerine eşit uzaklıkta olacak şekilde belirlenmelidir.\n",
    "\n",
    "Geometrik olarak, ayrımı sağlayan fonksiyonu $\\ w^T x = b $ olarak belirlersek $\\ b=0 $ olduğunda elde edilen $\\ x $ 'in alacağı her değerin $\\ w $ vektörüne dik olduğu görülür. Bu durumda belirlenen hiperdüzlemin sınıflandırma yapması beklenemez. Hiperdüzlemi orijinin $\\ a $ vektörü kadar uzağına taşırsak kullanılacak fonksiyon $\\ (x-a)^T w = 0 $  olacak şekilde değişir. \n",
    "$\\ b = a^T w $ buradan $\\ b $ 'nin, $\\ a $ 'nın vektör w üzerindeki görüntüsü olduğu çıkarımı yapılabilir. $\\ a $'yı düzleme dik olarak seçtiğimizde $\\ ||a||  = |b|  /  ||w|| $ uzunluğu hiperdüzlem ile orijin arasındaki en kısa uzaklıktır. \n",
    "\n",
    "Sınıflandırma için kullandığımız hiperdüzleme paralel, hiperdüzleme en yakın eğitim örneklerini her iki tarafta kesecek şekilde iki hiperdüzlem seçilir. Bu iki düzleme destek hiperdüzlemleri adı verilir. Hiperdüzlem ve destek hiperdüzlemleri arasındaki uzaklığı $\\ d_+ $ ve $\\ d_- $ olarak tanımlayalım. Hiperdüzlemler arasındaki uzaklık- marjin, $\\ \\gamma  $,  $\\ d_+ + d_- $ olarak bulunur. Uzaklığı maksimum yapacak şekilde, destek hiperdüzlemlerine eşit uzaklıkta bir ayırıcı hiperdüzlem bulmalıyız. Ayırıcı hiperdüzlemin denklenmini $\\ w^T x = b $ seçmiştik. Buradan yola çıkarak destek hiperdüzlemlerini $$ w^T x = b + \\delta $$\n",
    "$$ w^T x = b - \\delta $$ olarak tanımlayabiliriz.\n",
    "\n",
    "Elde etiğimiz eşitliklerde $\\ w $,$\\ b $ ve $\\ \\delta $ 'nın sabit bir $\\ \\alpha $ ile çarpılması $\\ x $ 'in eşitlikleri sağlamasını engellemez. Bu nedenle değişken sayısını azaltmak ve belirsizliği ortadan kaldırmak için $\\ \\delta = 1 $ diyebiliriz. Buradan, $$ d_+ = \\frac {|| b+1 | - | b ||}{|| w ||} ,  b \\notin \\{-1,0\\} $$ elde edilir. Orijinin hiperdüzlemler arasında kaldığı durum için ise $$ d_+ = \\frac {|| b+1 | + | b ||}{|| w ||} , b \\in \\{-1,0\\} $$ elde edilir. Bu yolla marjini, $\\ \\delta = \\frac {2}{||w||}$ olarak buluruz.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Bu eşitlikler göz önüne alındığında çizelecek hiperdüzlemler aşağıdaki çözümleri sağlamalıdır.\n",
    " \n",
    " İlk durum kullanılarak $\\ w^T x_i − b ≤ −1,  ∀ y_i = −1 $ çözümü elde edilir.\n",
    " \n",
    " İkinci durumun gerçeklenmesi ise $\\ w^T x_i − b ≥ +1,  ∀ y_i = +1 $ ile sağlanabilir.\n",
    " \n",
    " Birinci ve ikinci çözümlerin genelleştirilmesiyle elde edilen $\\ y_i (w^T x_i − b) − 1 ≥ 0 $    çözümü hiperdüzlemler için sağlanması gereken genel çözümdür.\n",
    " \n",
    " Böylece SVM'in primer problemini  \n",
    " \n",
    " $$ minimize_{w,b}   \\frac{1}{2} ||w||^2 $$ ve  $\\ subject $ $\\ to $ $\\ y_i (w^T x_i − b) − 1 ≥ 0 $ $\\ \\forall i $ \n",
    " \n",
    " \n",
    " \n",
    " şeklinde formülize edebiliriz. Bu yolla destek hiperdüzlemlerinin farklı taraflarında kalan eğitim durumlarına göre belirlenen kısıtlar yolu ile marjı maksimize edebiliriz. Hiperdüzlemleri destekleyen, sorunun çözümünün bulunmasını sağlayan hiperdüzlemlerin üzerindeki veri durumlarına destek vektörü denir.\n",
    " \n",
    "Kernelize etmeyi algoritma inputlarının verimliliğini artırmak için daha küçük input ile değiştirildiği daha verimli algoritma tekniği olarak tanımlayabiliriz.Primer problem çeşitli programlarla çözülebilmesine rağmen lineer olarak sadece veri vektörleri arasındaki iç çarpıma bağlı olmadığından kernelize edilmek için uygun değildir.  Bu nedenle elde ettiğimiz problemi dual forma dönüştürmeliyiz.\n",
    "$$ \\mathcal{L}(w,b, \\alpha ) = \\frac{1}{2} ||w||^2 - \\sum_{i=1}^N \\alpha_i [y_i (w^T x_i − b) − 1] $$\n",
    "\n",
    "$\\ min_w min_\\alpha \\mathcal{L}(w,\\alpha) $ kısıtları primer problemin minimum değerdeki çözümünü elde ederken kullanılır. Orijinal çözümdeki fonksiyon konveks olduğundan minimizasyon ve maksimizasyon durumlarını değiştirmeliyiz. Ancak eyer noktasını sağlayan yeni kısıtlar oluşturulmalıdır. Dual problemin $\\ w $ ve$\\ b $'ye göre türevi alındığında\n",
    "$$ w - \\sum_i \\alpha_i y_i x_i  \\Rightarrow  w^* = \\sum_i \\alpha_i y_i x_i   $$\n",
    "$$ \\sum_i \\alpha_i y_i = 0 $$\n",
    "\n",
    "\n",
    "Elde ettiklerimizi Lagrange denkleminde yerine koyarsak $$ maximize \\quad  \\mathcal{L}_D = \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{ij} \\alpha_i \\alpha_j y_iy_j x_i^Tx_j$$\n",
    "            $$ subject \\quad to \\quad \\sum_i \\alpha_iy_i =0 $$ $$ \\alpha_i \\geq 0, \\quad  \\forall i $$\n",
    "            \n",
    "Burada değişken sayısı olan $\\ \\alpha_i $, veri durumu sayısı olan $\\ N $'e eşittir. \n",
    "\n",
    "Problem sadece $\\ x_ix_j $ iç çarpımına bağımlıdır. $\\ x_i^Tx_j $ çarpımı $\\ k(x_i,x_j)$  şeklinde dual problem kernelize edilebilir.\n",
    "            \n",
    "Dualite teorisi konveks problemleri konkav hale getirerek dual problemle birbirine eş primer problem sonuçları elde edilmesini sağlar. Yani $\\ \\mathcal{L}_P(w^*)= \\mathcal{L}_D(\\alpha^*) $'dır.\n",
    "\n",
    "Eyer noktasının sağlayan yeni kısıtlar ve problemin çözümünde Karush-Kuhn-Tucker(KKT) koşullarını kullanılmalıdır. Bu koşullar genel olarak gerekli ve konveks optimizasyon problemleri için yeterlidir.\n",
    "\n",
    "Primer problemin $\\ w $ ve $\\ b $'ye göre türevi alınarak\n",
    "\n",
    "$$ ∂_bL_P = 0 \\rightarrow w − \\sum_i α_iy_ix_i = 0 $$\n",
    "$$ ∂_bL_P = 0 \\rightarrow  \\sum_i α_iy_i = 0 $$\n",
    "elde edilir.\n",
    "Primer problemin kısıtı olan $\\ kısıt-1 $ $$ y_i (w^T x_i − b) − 1 ≥ 0 $$\n",
    "Lagrange çarpanlarının negatif olmadığı eşitsizlik $\\ çarpan $$\\ koşulu $ için $$ \\alpha_i \\geq 0 $$ \n",
    "ve tümler gevşeklik koşulu $$ \\alpha_i [y_i (w^T x_i − b) − 1]=0 $$ sağlanmalıdır.\n",
    "\n",
    "Tümler gevşeklik koşulu, $\\ y_i (w^T x_i − b) − 1 > 0 $ iken $\\ \\alpha_i=0 $ olduğu veya $\\ \\alpha_i \\geq 0 $ iken $\\ y_i (w^T x_i − b) − 1 = 0 $ olduğu durumlarda sağlanır. Burada eşitsizlikler$\\ \\textit{aktif} $ kısıtlar, diğerleri ise aktif olmayan kısıtlardır. Çözüm bulunurken sonucu değiştirmediğinden aktif olmayan kısıtları kaldırabiliriz. \n",
    "\n",
    "Eğitim grubundaki destek vektörleri olan $\\ \\alpha_i > 0 $, destek hiperdüzlemleri üzerinde aktif kısıtlardır ve çözümü sağlarlar. Ancak bir çoğu hiperdüzlemleri sıfırlarlar. Bu da seyrek olmalarına neden olur.\n",
    "$$ f(x) = w^{*T}x - b^* = \\sum_i \\alpha_i y_i x_i^Tx - b^* $$\n",
    "fonksiyonu gelecekteki test durumlarını sınıflandırmak için kullanılacaktır.\n",
    "\n",
    "KKT koşullarından tümler gevşeklik koşulunu kullanarak $$ b^* = ( \\sum_j \\alpha_j y_j x_j^T x_i - y_i) $$  olarak bulunur. Burada her $\\ i $ bir destek vektörünü simgeler ve $\\ y^2=1 $ . Bu yolla herhangi bir destek vektörünü kullanarak $\\ b $'yi bulabiliriz ancak sayısal anlamda dengeyi sağlamak için ortalamalarını almak tercih edilebilir.\n",
    "\n",
    "Ulaşılan en önemli sonuç $\\ f(x) $ fonksiyonunun $\\ x^T_i x_i $ ile ifade edilebileceğidir; bu da, kernel matrisleri $\\ k(x_ix_j) $ ile yer değiştirebilir ve yüksek boyutlu doğrusal olmayan uzaylarda da kullanılabilir olduğunu gösterir. Ayrıca $\\ \\alpha $ seyrek olduğundan yeni input $\\ x $'leri tahmin etmek için fazla sayıda kernel değerlendirmeye gerek kalmamıştır.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[Kaynak 6: A First Encounter with Machine Learning Max Welling Donald Bren School of Information and Computer Science University of California Irvine April 21, 2010](https://www.ics.uci.edu/~welling/teaching/273ASpring10/IntroMLBook.pdf)\n",
    "\n",
    "\n",
    "##### Scikit-learn kütüphanesinde SVM  \n",
    "\n",
    "Scikit-learn Python diliyle çalışan açık kaynaklı bir makine öğrenmesi kütüphanesidir. Veri analizi ve veri madenciliği için verimli bir araç olan sckit-learn; NumPy, SciPy ve matplotlib sayısal ve bilimsel kütüphaneleri ile birlikte çalışmak üzere tasarlanmıştır. [Kaynak 7](http://scikit-learn.org/stable/index.html)\n",
    "\n",
    "\n",
    "Scikit-learn'de çoklu sınıflandırma SVC, NuSVC ve LinearSVC sınıfları ile yapılabilir. SVC vr NuSVC benzer metodlardır ancak parametre setleri birbirinden biraz farklıdır ve matematiksel formülasyonları farklıdır. Diğer taraftan LinearSVC destek vektör makineleri için kullanılan bir diğer metoddur. LinearSVC doğrusal kernel ile çalıştığından kernel parametresi kullanılmaz.  \n",
    "\n",
    "###### SVC\n",
    "\n",
    "$\\{x_i, y_i\\}$, $i=1,...,n$ ve $x_i \\in \\mathbb{R}^d$ ve $y_i \\in \\{−1, +1\\}$ iken burada $\\ x_i $ eğitim vektörleri ve $\\ y_i $ bağımlı değişkenlerdir. Açılımı C-Support Vector Classification olan SVC primal problemi \n",
    "$$ min_{w,b,\\zeta} \\quad \\frac{1}{2}w^Tw + C \\sum_{i=1} \\zeta_i $$\n",
    "$$ subject \\quad to \\quad y_i(w^T \\phi(x_i) + b) \\geq 1-\\zeta_i $$\n",
    "$$ \\zeta_i \\geq 0, i=1,2,..,n $$ olarak tanımlar.\n",
    "\n",
    "Dual problem ise $$ min_a \\frac{1}{2} \\alpha^T Q \\alpha - e^T \\alpha $$\n",
    "$$ subject \\quad to \\quad y^T \\alpha = 0 $$\n",
    "$$   \\qquad 0 \\leq \\alpha_i \\leq C, \\quad i=1,2,...,n $$\n",
    "burada $\\ e $ bütün elemanları 1 olan vektör, $\\ C >0 $ üst sınır, $\\ Q $ $\\ nxn $ simetrik matris, $\\ Q_{ij} \\equiv y_iy_j K(x_i,x_j) $'dir. Kernel $\\ K(x_ix_j) = \\phi(x_i)^T \\phi(x_j) $. Böylece $\\ \\phi $ fonksiyonu yardımıyla eğitim vektörleri daha yüksek boyutlu uzayda incelenir.\n",
    "\n",
    "Elde edilen gözlemlere dayanarak istatiksel kurallar uygulanarak $\\ sgn( \\sum_{i=1}^n y_i \\alpha_i K(x_i,x) + \\rho ) $ elde edilmiştir. libsvm ve liblinear kütüphaneleri SVM modellerinde kullanılırken $\\ C $ parametresi kullanılırken, diğer kütüphaneler için $\\ \\alpha $ kullanılır. $\\ C $ ile $\\ \\alpha $ arasındaki ilişki $\\ C = \\frac {n_{örnek}}{\\alpha} $ şeklindedir.\n",
    "$\\ y_i \\alpha_i $ çarpımına dual_coef_ , $\\ \\rho $ bağımsız değişkenine intercept_ parametreleri yoluyla ulaşılabilir. \n",
    "[Kaynak 8](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.7215)\n",
    "[Kaynak 9](https://link.springer.com/article/10.1007%2FBF00994018)\n",
    "[Kaynak 10](http://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation)\n",
    "\n",
    "###### NuSVC\n",
    "\n",
    "NuSVC metodu ile destek vektör sayısını ve eğitim hatasını kontrol eden$\\ v $ parametresi kullanılmaya başlanmıştır. Eğitim hatası, eğitimli modeli eğitim verilerine uyguladığımızda elde edilen hatadır. $\\ v \\in (0,1] $ parametresi eğitim hataları oranı için üst sınır, destek vektör oranı için alt sınırdır.\n",
    "$\\ v $-SVC formülasyonu $\\ C $-SVC'nin yeniden parametrize edilmiş şeklidir dolayısıyla matematiksel olarak eşitidir.\n",
    "[Kaynak 11](http://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation)\n",
    "\n",
    "###### SVR\n",
    "\n",
    "SVR ise  $x_i \\in \\mathbb{R}^d$ test vektörleri ve $y_i \\in \\mathbb{R}^n$ vektörü üzerinde tanımlıdır ve primal problemi\n",
    "$$ min_{w,b,\\zeta,\\zeta^*} \\quad \\frac{1}{2}w^Tw + C \\sum_{i=1} (\\zeta_i+\\zeta_i^*) $$\n",
    "$$ subject \\quad to \\quad y_i(w^T \\phi(x_i) + b) \\leq \\varepsilon+\\zeta_i $$\n",
    "$$ w^T \\phi(x_i)+b-y \\leq \\varepsilon+\\zeta_i^*  $$\n",
    "$$ \\zeta_i, \\zeta_i^* \\geq 0, \\quad i=1,2,..,n$$\n",
    "\n",
    "şeklinde çözer.\n",
    "\n",
    "Dual problem ise $$min_{ \\alpha, \\alpha^*}(\\alpha-\\alpha^*)^TQ(\\alpha-\\alpha^*)+ \\varepsilon e^T(\\alpha+ \\alpha^*)-y^T(\\alpha -\\alpha^*)$$\n",
    "$$subject\\quad to\\quad e^T(\\alpha-\\alpha^*)=0 $$\n",
    "$$0 \\leq \\alpha_i,\\quad \\alpha_i^* \\leq C,\\quad i=1,2,...,n$$ şeklinde tanımlanır. Burada $\\ e$ bütün elemanları 1 olan matris, $\\ C>0$ üst sınır, $\\ Q $ ise $\\ nxn$ pozitif yarı-tanımlı matris ve $\\ Q_{i,j} \\equiv K=(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)$ kerneldir. Eğitim vektörleri çok boyutlu uzayda $\\ \\phi $ fonksiyonu yardımıyla tanımlanabilirdir.\n",
    "Elde edilen gözlemlere dayanarak istatiksel kurallar uygulanarak $\\ \\sum_{i=1}^n(\\alpha-\\alpha^*)K(x_i,x)+\\rho $ elde edilmiştir.\n",
    "\n",
    "$\\ \\alpha_i-\\alpha_i^* $ farkına dual_coef_, destek vektörleri support_vectors_ ve $\\ \\rho $ bağımsız değişkenine intercept_ parametreleri yoluyla ulaşılabilir.\n",
    "[Kaynak 12](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.114.4288)\n",
    "[Kaynak 13](http://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation)\n",
    "\n",
    "Çok sınıflı sınıflandırma yapılırken SVC ve NuSVC “one-against-one” [Kaynak 14](https://link.springer.com/chapter/10.1007/978-3-642-76153-9_5) yaklaşımını benimsemiştir. Bu yaklaşıma göre $\\ n_class $ sınıf sayısı olmak üzere $\\ \\frac {n_{sınıf}*(n_{sınıf}-1)}{2} $ sınıflandırıcı oluşturulur. Bu sınıflandırıcıların her biri iki ayrı sınıftan veriyi eğitirler. \n",
    "\n",
    "LinearSVC ise “one-vs-the-rest” stratejisini benimsemiştir. Bu strateji sınıf başına tek bir sınıflandırıcı eğitimini içerir; bir sınıfa ait veriler pozitif, diğer tüm veriler negatif kabul edilir.[Kaynak 15](http://s3.amazonaws.com/academia.edu.documents/30428242/bg0137.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1494454268&Signature=aPw223N7eRJQ2KV2Hgrj%2BKYveUw%3D&response-content-disposition=inline%3B%20filename%3DPattern_recognition_and_machine_learning.pdf)\n",
    "LinearSVC'nin multi-class ='crammer_singer' parametresi kulanılarak çok sınıflı SVM olarak adlandırılan başka bir strajesi daha bulunur. Bu sınıflandırma one-vs-the-rest yaklaşımına göre daha düşük çalışma zamanına sahiptir.\n",
    "\n",
    "\n",
    "#### Karar Ağaçları\n",
    "\n",
    "Yorumlama için basit ve kullanışlı yöntemlerden olan Ağaç Tabanlı Yöntemler(Tree Based Methods) yüksek doğruluk, kararlılık ve yorumlanma kolaylığı ile tahmini modelleri güçlendirir. Verilen belirli bir gözlem için tahmin yapmak amacıyla genellikle eğitim gözlemlerinin ortalaması ya da ait olduğu bölgedeki modu kullanılır. Tahmini alanı bölmek için kullanılan bölme kuralları kümesi birağaçta özetlenebilir ve bu tip yaklaşımlar karar ağaçları olarak adlandırılır. Karar ağaçlarının otomatik olarak oluşturulması, Morgan & Sonquist (1963) ve Morgan & Messenger (1973) tarafından sosyal bilimler alanında yapılan çalışmalara dayanır.( kaynak1-kitap :W.N. Venables B.D. Ripley Modern Applied Statistics with S-PLUS Third Edition)\n",
    "\n",
    "Karar ağaçları hem regresyon hem de sınıflandırma problemlerine uygulanabilir. Birden fazla bağımsız parametre ve bir bağımlı parametreiçeren data setinde, bağımlı parametre kategorik ise sınıflandırma ağacı, sürekli yapıda ise regresyon ağacı adını alır.Yani, her birgözlemin birçok farklı grup veya sınıftan birinden geldiği bilinmektedir ve amaç etiketlenmemiş gözlemleri sınıflandırmak içinöngördürücü değişkenleri kullanmaktır. Bazı durumlarda, analist hangi öngördürücülerin yanıtla ilişkili olduğunu, bunların nasıl ilişkili olduğunu ve belki de öngörücü değişkenlerin bile başkaları ile tepki tahmininde nasıl etkileşime girdiğini belirlemek isteyebilir. (kaynak kitap chapter 5:High-Dimensional Data Analysis in Cancer Research)\n",
    "\n",
    "Amaç hem kategorik hem nümerik verilerden faydalanarak çıkarılan basit karar kuralları ile bir hedef değişkeninin değerini öngören model oluşturmaktır.\n",
    "Karar Ağaçlarının Avantajları ve Dezavantajları\n",
    "Karar Ağaçlarını Bazı Avantajları:\n",
    "+ Çok fazla veri hazırlığı gerektirmez. Diğer teknikler genellikle veri normalleştirme gerektirir, model değişkenlerin oluşturulması ve boş değerlerin kaldırılması gerekir. Bununla birlikte, bu algoritma eksik değerleri desteklemez.\n",
    "+ Anlaması ve yorumlaması basit olan ağaçlar görselleştirilebilir.\n",
    "+ Hem sayısal hem de kategorik verileri işleyebilir. Diğer teknikler genellikle yalnızca bir tür değişken içeren veri kümelerini analiz etmede uzmanlaşmıştır.\n",
    "+ İstatistiksel testler kullanarak bir modelin doğrulanması mümkündür. Bu, modelin güvenilirliğini hesaba katmayı mümkün kılar.\n",
    "Karar Ağaçlarının Bazı Dezavantajları:\n",
    "+ Karar ağacı öğrenenleri, veriyi genelleştirmeyen aşırı karmaşık ağaçlar oluşturabilir. Buna aşırı uyma denir. Şu anda desteklenmeyen budama gibi mekanizmalar bir yaprak düğümünde gerekli asgari örnek sayısını ayarlamak veya ağacın maksimum derinliğini ayarlamak bu sorundan kaçabilmek için gereklidir.\n",
    "+ Karardaki ağaçları sabit olmayabilir, çünkü verilerdeki küçük değişiklikler tamamen farklı bir ağacın oluşmasına neden olabilir.\n",
    "+ Özellik değişkenleri, potansiyel bölme noktaları sayısı ve ağacın geniş derinliği nedeniyle, aynı veri girişi kümesindeki toplam ağaç sayısı düşünülemez derecede güçtür. Bu nedenle, ağaç bölme sadece küresel değil, aynı zamanda küresel olarak en uygun ağacın hesaplanması pratik olarak imkansızdır.\n",
    "\n",
    "\n",
    "Karar Ağaçları Terminolojisi\n",
    "\n",
    "\n",
    "\n",
    "1. Kök Düğüm: Tüm popülasyonu veya alınmış örneği temsil eder ve iki ya da daha fazla homoje kümeye ayrılır.\n",
    "2. Yaprak Düğüm:Düğümler bölünmüyorsa yaprak düğüm adını alır.\n",
    "3. Karar Düğümü: Bir alt düğüm başka alt düğümlere ayrılırsa, karar düğümü olarak adlandırılır.\n",
    "4. Ata ve Çocuk Düğümü:Belli bir düğümün herhangi bir alt düğümüne çocuk düğüm denir ve verilen düğüm sırasıyla çocuğun üstüdür.\n",
    "5. Alt Ağaç: Ağacın bir alt bölümüne alt ağaç adı verilir.\n",
    "6. Bölme: Bir düğümün iki veya daha fazla alt düğümlere bölünmesi işlemidir.Bunlar karar ağaçları için kullanılan yaygın terimlerdir.\n",
    "\n",
    "Sınıflandırma Ağacı Sınıflandırma ağaçları (Classification Trees) 1984 yılında Leo Breiman, Jerome Friedman, Richard Olshen ve Charles Stone tarafından geliştirilen bir yöntemdir. Parametrik olmayan bu yöntem veri madenciliğinde oldukça önemli bir yere sahiptir( kaynak Breiman, L., 2001, Random forests, Machine Learning, 45, 5-32 p).Parametrik olmayan istatistikler ('dağılımsız istatistikler') de denir), bir popülasyonun bazı özelliklerini tanımlayabilen, bu nitelik hakkında hipotezleri test edebilen, diğer bir özellikle olan ilişkisini veya zaman içinde ilgili yapılardaki bu özellikteki farklılıkları test edebilen, popülasyonun veri dağılımı hakkında hiçbir varsayıma ihtiyaç duymayan ve aralık düzeyinde ölçüm gerektirmeyen bir yöntemdir. [Kaynak 18](http://psych.unl.edu/psycrs/971/nonpar/intro.pdf). Kantitatif bir cevap yerine nitel bir cevabı tahmin etmek için kullanılan Sınıflandırma Ağaçlarında sınıf daha önceden atanmış bir değerdir.(kaynak: chapter 8.1.2 An Introduction to Statistical Learning with Applications in R ). Bir sınıflandırma ağacı, sıralı bir dizi soruyu sormanın sonucudur ve dizideki her adımda sorulan soru türü, dizinin önceki sorularına verilen cevaplara bağlıdır.Sınıflandırma ağacında eğitim verisinde terminal düğüm tarafından elde edilen değer o bölgedeki gözlemlerin modudur. Böylece, görünmeyen bir veri gözlemi o bölgede düşerse, tahminini mod değeri ile yapacağız. Tahmini alanı belirgin ve çakışmayan bölümlere ayırır. Basit olması adına, bu bölgeleri yüksek boyutlu kutular olarak düşünebiliriz.Stratejik bölünmeler yapma kararı bir ağacın doğruluğunu büyük ölçüde etkiler. Bölünme, kullanıcı tarafından algoritmada tanımlanan bir durdurma kriterine ulaşılana kadar sürer.Bu bölme işlemi durdurma kriterlerine ulaşılana kadar tamamen yetişmiş ağaçlarda sonlanır.\n",
    "\n",
    "Bir ağaç, ilk önce tüm gözlemleri içeren bir 'kök' düğümü düşünülerek yetiştirilir, bu düğümdeki gözlemler tek bir öngörücü değişkende bir 'bölme' kullanılarak bir sol ve bir sağdaki iki alt nesne düğümünden birine gönderilir( kaynak High-Dimensional Data Analysis in Cancer Research chapter 5).Bir düğüm, değişken kümesinin alt kümesidir ve bir terminal olabilir veya terminal olmayabilir. Bir nonterminal (veya ana) düğüm, iki kız düğümüne bölünen bir düğümdür(ikili bölme).\n",
    "\n",
    "Bölünmeyen bir düğüme bir terminal düğümü denir ve bir sınıf etiketi atanır. Kategorik bir öngördürücü değişkeni için, bir bölme sola bir alt grup gönderir ve geri kalanını ise sağa gönderir.Bir ağacın bir düğümü iki soyuna bölmek için kullandığı belirli bölme, her tahmini değişkende olası tüm bölünmeleri göz önüne alarak seçilir. Bazı ölçütlere göre 'en iyi' değeri veren tahmin edici ve bölünmüş kombinasyon, düğümü bölmek için kullanılır. Aynı prosedür, bazen 'özyinelemeli bölümleme' olarak adlandırılan soyundan gelen düğümlere uygulanır. Kategorik tahminci değişkenlerin kullanıldığı sınıflandırma ağaçları için bölünme kuralı, C kategori altkümesine bağlıdır. ($\\ x_i $ ∈ C) olan gözlemler sol alt düğüme atanırken ($\\ x_i $ ∉ C ) koşuluna uyan gözlemler ise sağ alt düğüme yerleştirilir. Ağaç oluştururken düğümleri bölerek alt düğümler oluşturmada en çok kullanılan kurallar aşağıdaki gibidir:\n",
    "Sınıflandırma ağacı girdi olarak iki dizi alır: alıştırma örnekleri tutan seyrek veya yoğun bir dizi ve boyutu [n_samples, n_features] olan X dizisi, alıştırma örnekleri için sınıf etiketlerini tutan ve tamsayı değerler alan Y boyut [n_samples] dizisi.\n",
    "Veriyi genelleştirmeyen aşırı karmaşık ağaçlar oluşması, verideki küçük bir değişikliğin tamamen farklı bir yapı oluşturması, öğrenmesi zor kavramlar içermesi gibi nedenler karar ağaçlarının dezavantajlarındandır. En ayırt edici veriyi belirlemek için bilgi kazancı ölçülür. Bilgi kazancı ölçümünde entropi kullanılır.\n",
    "\n",
    "\n",
    "\n",
    "Satır sınıflandırmak için gerekli bilgi :  $$I(s_1,s_2,...,s_m)=-\\sum_i  \\frac {si}{s}log_2 \\frac {si}{s}, \\quad\n",
    "i=1,2,...,m $$\n",
    "\n",
    "Entropi=$$ E(C|A_K)=  \\sum_{j=1}^{M_K}  p(a_{k,j})*[- \\sum_{i=1}^{N}  p(c_i|a_{k,j})log_2p(c_i|a_{k,j})] $$\n",
    "\n",
    "Bu formülde \n",
    "+  $\\ E(C|A_K)= A_K \\quad alanının \\quad sınıflama \\quad özelliğinin \\quad Entropi \\quad ölçüsüdür. $ \n",
    "+ $\\ p(a_{k,j})=a_k \\quad alanının \\quad j \\quad değerinde \\quad olma \\quad olasılığı $\n",
    "+ $\\ p(c_i|a_{k,j})= a_k \\quad alanı \\quad j. \\quad değerindeyken \\quad sınıf \\quad değerinin \\quad c_i \\quad olma \\quad olasılığı $\n",
    "+ $\\ M_k = a_k \\quad alanının \\quad içerdiği \\quad değerlerin \\quad sayısı; \\quad j = 1,2,., M_k $\n",
    "+ $\\ N = farklı \\quad sınıfların \\quad sayısı; \\quad i = 1, 2,…, N $\n",
    "+ $\\ K = niteliklerin \\quad sayısı; \\quad k = 1,2,…, K $\n",
    "\n",
    "Eğer bir $\\ S $ kümesindeki elemanlar, kategorik olarak $\\ C_1,C_2,C_3,...,C_i $ sınıflarına ayrıştırılırsa S kümesindeki bir elemanın sınıfını belirlemek için gereken bilginin(information) formülü şu şekildedir:\n",
    "\n",
    "\n",
    "$$ I(s_1,s_2,....s_m)=- \\sum _{i=1}^{m} p_ilog_2(p_i) $$\n",
    "\n",
    "Bu formülde $\\ p_i,\\quad C_i $ sınıfına ayrılma olasılığıdır. \n",
    "Entropi hesaplamak için kullanılabilecek bir başka formül:\n",
    "$$ E(A)=  \\sum_{i=1}^{n} \\left( \\frac{S_i}{S} \\right) I(s_i) $$\n",
    "Bu durum için A niteliği kullanılarak yapılacak dallanma işleminde, bilgi kazancı(information gain) aşağıdaki gibi hesaplanmaktadır.\n",
    "$$ Kazanç(A)=E(S)-E(A) $$ \n",
    "Burada $\\ E(S) $; sistemin entropi değeri olup $\\ E(A) $ ise A niteliğinin entropi değeridir.\n",
    "Bilgi kazancı her nitelik için ayrı ayrı hesaplanır ve bilgi kazancı en yüksek olan nitelik ağacın en üstünde konumlanarak kök düğüm olmuş olur.\n",
    "Bu işlemler her düğüm için örneklerin hepsi aynı sınıfa aitse, örnekleri bölecek özellik kalmamış,kalan özelliklerin değerini taşıyan örnek olmayana kadar devam eder. Bütün bu işlemler tamamlandığında karar agacı tamamlanmış olur.\n",
    "\n",
    "[Kaynak 21](www.ayhandemiriz.com/SakaryaWebSite/slides/DMHafta5.ppt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Veri Seti\n",
    "\n",
    "Cam Tanılama Veritabanı\n",
    "Kaynaklar: https://archive.ics.uci.edu/ml/datasets/Glass+Identification\n",
    "\n",
    "Yaratıcı:\tB. German\n",
    "        -- Central Research Establishment\n",
    "           Home Office Forensic Science Service\n",
    "           Aldermaston, Reading, Berkshire RG7 4PN\n",
    "\n",
    "Donör: Vina Spiehler, Ph.D., DABFT\n",
    "               Diagnostic Products Corporation\n",
    "               (213) 776-0180 (ext 3014)\n",
    "\n",
    "Tarih: Eylül 1987\n",
    "\n",
    "Vina, yararlı bir şekilde bilgiyi depolamak ve yönlendirmek için kullanılan kurala dayalı sistem olan BEAGLE'ı oluşturdu.BEAGLE, en yakın komşu algoritması ve diskriminant analizi ile karşılaştırma testi yapılarak camın cam türünü belirlemek için kullanıldı. Şu anda BEAGLE, VRS Consulting, Inc tarafından satılan bir üründür; 4676 Amiral Yolu, Takım 206; Marina Del Ray, CA 90292 (213) 827-7890 ve Faks: -3189.\n",
    "\n",
    "Cama uygulanan float işlemi, camın eritilerek yüzdürülmesiyle elde edilmesi olarak tanımlanabilir.\n",
    "\n",
    "Cam çeşitlerinin sınıflandırılması üzerine yapılan çalışma, suçun gerçekleştirildiği yer olan suç mahallinde kalan camların doğru olarak tanımlanması ve kanıt olarak kullanılabilmesi amacıyla yapılmıştır.\n",
    "Instance sayısı: 214\n",
    "Attribute sayısı:10 \n",
    "Bu attributelara bakacak olursak:\n",
    "\n",
    "1.\tId numarası:1-214 arasında unique bir integer değeri alır.\n",
    "2.\tRI(refractive index): Kırılma endeksi, n malzemeden oluşmuş bir materyal için ışığın maddeden geçişindeki kırılma oranını gösterir. Float bir değer alır.\n",
    "3.\tNa: Bir birim ağırlığa karşılık gelen oksitin içindeki Sodyum’un ağırlık yüzdesidir. Float bir değer alır.\n",
    "4.\tMg: Bir birim ağırlığa karşılık gelen oksitin içindeki Magnezyum’un ağırlık yüzdesidir. Float bir değer alır.\n",
    "5.\tAl: Bir birim ağırlığa karşılık gelen oksitin içindeki Aluminyum’un ağırlık yüzdesidir. Float bir değer alır.\n",
    "6.\tSi: Bir birim ağırlığa karşılık gelen oksitin içindeki Silikon’un ağırlık yüzdesidir. Float bir değer alır.\n",
    "7.\tK: Bir birim ağırlığa karşılık gelen oksitin içindeki Potasyum’un ağırlık yüzdesidir. Float bir değer alır.\n",
    "8.\tCa: Bir birim ağırlığa karşılık gelen oksitin içindeki Kalsiyum’un ağırlık yüzdesidir. Float bir değer alır.\n",
    "9.\tBa:Bir birim ağırlığa karşılık gelen oksitin içindeki Baryum’un ağırlık yüzdesidir. Float bir değer alır.\n",
    "10.\tFe:Bir birim ağırlığa karşılık gelen oksitin içindeki Demir’in ağırlık yüzdesidir. Float bir değer alır.\n",
    "11.\tCam türü (sınıf attribute)\t\n",
    "    1. building_window_float_processed: Binalarda kullanılan eritilmiş camın yüzdürülerek işlem görmesinden oluşan modern pencere camıdır.    \n",
    "    2. building_windows_non_float_processed: Binalarda kullanılan yüzdürme işleme görmeden oluşturulan pencere camıdır.\t\n",
    "    3. vehicle_windows_float_processed: Araçlarda kullanılan eritilmiş camın yüzdürülerek işlem görmesinden oluşan modern pencere camıdır.\n",
    "    4. vehicle_windows_non_float_processed: Araçlarda kullanılan normal pencere camdır. Fakat bu sınıf elimizdeki veri setinde yer bulamamıştır.\n",
    "    5. containers: konteyner\t\n",
    "    6. tableware:Sofra takımlarında kullanılan camdır.\n",
    "    7. headlamps:Araçlarda kullanılan far camıdır.\n",
    "    \n",
    "    \n",
    "    \n",
    "Yapılan araştırma sonucunda camın, float işlemiyle üretilmiş bir cam türü olup olmadığını belirlemede aşağıdaki sonuçlar elde edilmiştir:    \n",
    "    \n",
    "|  Örnek tipi                                           | BEAGLE        | En Yakın Komşu  | Diskriminant Analizi |\n",
    "| ------------------------------------------------------|:-------------:|:---------------:|:--------------------:|\n",
    "| Float işlemiyle üretilmiş cam sayısı: 87              | 10            | 12              | 21                   |\n",
    "| Float işlemi kullanılmadan üretilmiş cam sayısı: 76   | 19            | 16              | 22                   | \n",
    "\n",
    "\n",
    "\n",
    "Bu çalışma sonucunda ortaya çıkan istatistiğin özeti\n",
    "\n",
    "| Attribute | Minimum | Maksimum | Ortalama | Serbestlik derecesi | Sınıfla korelasyon |\n",
    "| --------- | :-----: | :------: |:-------: | :----------------:  | :----------------: |\n",
    "| 2. RI     |  1.5112 | 1.5339   |1.5184    |  0.0030             | -0.1642            |\n",
    "| 3. Na     | 10.73   | 17.38    |13.4079   |  0.8166             | 0.5030             |\n",
    "| 4. Mg     | 0       | 4.49     |2.6845    |  1.4424             | -0.7447            |\n",
    "| 5. Al     | 0.29    | 3.5      |1.4449    |  0.4993             | 0.5988             |\n",
    "| 6. Si     | 69.81   | 75.41    |72.6509   |  0.7745             | 0.1515             |\n",
    "| 7. K      | 0       | 6.21     |0.4971    |  0.6522             | -0.0100            |\n",
    "| 8. Ca     | 5.43    | 16.19    |8.9570    |  1.4232             | 0.0007             |\n",
    "| 9. Ba     | 0       |3.15      |0.1750    |  0.4972             | 0.5751             |\n",
    "| 10. Fe    | 0       |0.51      |0.0570    |  0.0974             | -0.1879            |\n",
    "\n",
    "\n",
    "214 örneklem ile elde edilen sonuç aşağıdaki gibidir:\n",
    "\n",
    "+ 163 pencere camından(bina ve cam pencecereleri) 87'si float işlemiyle elde edilmiş olup, bunların 70'i bina pencere camı, 17'si araba penceresi camıdır. Kalan 76'sı float işlemi uygulanmadan elde edilen pencere camı olup, tamamı bina pencere camıdır.\n",
    "+ Örneklemdeki 51 cam, pencere camı değildir. Bunların 13'ü konteynır, 9'u sofra takımlarında kullanılan cam ve 29'u araçlarda kullanılan far camıdır. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uygulama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Csv formatındaki veri setini aktarmak için $\\ pandas $ kütüphanesinden yararlanılmıştır. $\\ pandas $, veri analizinde kullanıldığında yüksek performans gösteren açık kaynak kodlu bir araçtır. Csv, txt, HDF5 ve SQL veritabanı gibi çeşitli formatlarda veri okuma ve yazma işlemlerinde kullanılabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "glass=pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "$\\ read\\_csv: $ Çeşitli formatlardaki dosyaları okumak için kullanılır. Parametre olarak URL ve dosya yolu alır. \n",
    "$\\ head $ parametresi; varsa veri setindeki sütun isimlerini alır bu durumda header=0 olur. Eğer sütun isimleri bulunmuyorsa header=None değerini alır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = glass.iloc[:, 1:10].values\n",
    "y = glass.iloc[:, 10].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ iloc: $ Veri seti içinde belirlenen tamsayı aralığındaki değerler ile dizi oluşturur. Genellikle sayısal değerli dizilerle kullanılmaktadır fakat mantıksal değerli diziler için de kullanılabilir. \n",
    "Veri setimizde ilk sütun index olduğundan bağımsız değişkenlerimiz X'ler için oluşan dizinin ikinci elemanı olan [1] ile [10] arasındaki değerler atanır. [10] ise bağımlı değişken y'nin veri setindeki 11. sütun değerlerini temsil eder.\n",
    "[Kaynak 22](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iloc.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.65, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Burada Scikit-learn kütüphanesinde $\\ model\\_selection.train\\_test\\_split $ kullanılarak bağımsız değişkenler ve bağımlı değişkenler, eğitim seti ve daha sonra gerçek değerlerle karşılaştırılacak olan test seti şeklinde bölünmüştür. Parametre olarak dizileri ve çeşitli seçenekleri alır.\n",
    "Bunlardan biri olan $\\ test\\_size $ parametresi float, integer veya None değerlerini alabilir. Aldığı defaut değer None'dır. Eğer float bir değer belirlenirse 0.0-1.0 arasında olan ve test setine dahil edilecek veri setinin oranını temsil eden bir değer alır. Belirlenen değer integer ise test örneklerinin mutlak sayısı olan bir değer alır. Parametrenin değeri default değer olan None olarak belirlenirse 0.25 olarak belirlenir.\n",
    "X_train, bağımsız değişkenlerden oluşan öğrenme setini; y_train, bağımlı değişkenlerden oluşan öğrenme setini ifade eder.\n",
    "X_test, bağımsız değişkenlerin oluşturduğu test seti; y_test, bağımlı değişkenlerin oluşturduğu test setidir.\n",
    "X ve y parametreleri daha önce veri setini ayırırken belirlediğimiz bağımlı ve bağımsız değişkenleri içeren dizilerdir.\n",
    "$\\ random\\_state $ parametresi rasgele örnekleme oluşturmak için kullanılan rastgele sayı üreticidir.\n",
    "[Kaynak 23](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ $\\ preprocessing.StandardScaler $ sınıfı birim varyansa göre ortalamayı ve ölçeklendirmeyi kaldırarak normalizasyonu sağlar. Yani bağımsız değişken setindeki alabilecekleri değer aralığını standartlaştırmak için kullanılır. Yeniden ölçeklendirma için genel olarak $\\ x'= \\frac{x-min(x)}{max(x)-min(x)} $ formülü kullanılır.[Kaynak 24](https://www.wikiwand.com/en/Feature_scaling) \n",
    "+ $\\ fit\\_transform $, parametre olarak verilen veri setinin ortalamasını daha sonra kullanılacak ölçeklendirme için hesaplar ve dönüştürür.\n",
    "+ $\\ transform $ ise ölçeklendirme ile standartlaştırmayı sağlar.[Kaynak 25](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Svm'in veri setine uygulanması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier=SVC(kernel='linear', random_state=0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC sınıfının aktarılmasından sonra sınıflandırıcı belirlenir. SVC parametreleri şu şekildedir:\n",
    "+ $\\ C: $ Penalty parametresi olan C hata terimidir. Float değerler alır. Varsayılan değeri 1.0'dir.\n",
    "+ $\\ kernel: $ Algoritmada kullanılacak kernel türünü belirler. String bir değer alır. Varsayılan değeri rbf'dir. Bunun dışında 'linear','poly', 'sigmoid' ve 'precomputed' değerlerini de alabilir.\n",
    "+ $\\ degree: $ 'poly' kernel fonksiyonunun derecesini integer bir değer olarak alır. Varsayılan değeri 3'tür.\n",
    "+ $\\ gamma: $ 'rbf', 'sigmoid' ve 'poly' için kernel katsayısıdır. Float bir değer alır. Varsayılan değeri auto'dur ve auto değeri de $\\ \\frac{1}{bağımsız \\quad değişken \\quad sayısı} $'na eşittir.\n",
    "+ $\\ random\\_state: $ İnteger değer alır. Varsayılan değeri None'dır. Yukarıda X ve y değişkenlerini eğitim ve test seti olarak $\\ train\\_test\\_split $ kullanarak bölmüştük. $\\ random\\_state $ bir integer değere eşitlenerek kullanılmaması durumunda bölünme kodu her çalıştırıldığında bölünme sonucu farklı olacaktır.[Kaynak 26](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "+ $\\ SVC $ sınıfının bir metodu olan $\\ fit $ verilen X ve y eğitim setlerine göre SVM modelini oluşturur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ $\\ predict: $ Bağımsız değişkenlerin test setine göre y değişkenleri modelini SVM'i kullanarak oluşturur.\n",
    "[Kaynak 27](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "SVM modelinin tahmin kalitesini değerlendirmek için $\\ metric \\quad functions $ yaklaşımı kullanıldı. Bu modül belirli amaçlar için tahmin hatasını değerlendiren metodları uygular.[Kaynak 28](http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) Burada sınıflamanın doğruluğunu değerlendirmek için $\\ confusion\\_matrix $ kullanıldı. Confusion matrix tanımına göre $\\ C_{ij} $, $\\ i $ grubunda olduğu bilinen fakat grup $\\ j $'de olduğu tahmin edilen gözlem sayısına eşittir. \n",
    "+ Parametre olarak dizi alır. Burada y'nin gerçek değeri ve tahmin edilen değerini parametre olarak vererek iki dizinin karşılaştırmasını yapar.\n",
    "+ Sonuç olarak elde edilen $\\ C,\\quad n $ sınıflı veri seti için $\\ nxn $ bir matristir.\n",
    "\n",
    "[Kaynak](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35 13  0  0  0  0]\n",
      " [12 31  0  6  3  0]\n",
      " [ 7  7  0  0  0  0]\n",
      " [ 0  0  0  4  0  0]\n",
      " [ 0  0  0  0  3  0]\n",
      " [ 1  0  0  1  0 17]]\n"
     ]
    }
   ],
   "source": [
    "print (cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elde ettiğimiz matris şu şekilde yorumlanabilir:\n",
    "+ Test setimizde 48 adet 1. türde cam vardır. SVM bunların 35 adetini 1. tür, 13 adetini 2. tür olarak tahmin etmiştir.\n",
    "+ Test setimizde bulunan 2. türdeki toplam 52 örneğin 31'i doğru tahmin edilmiştir. Bunların 12'si 1. tür, 6'sı 4.tür ve 3'ü 5. tür olarak tahmin edilmiştir.\n",
    "+ 14 adet bulunan 3. türdeki camın 7'si 1. tür, diğer 7'si 2. türdür.\n",
    "+ 4.türdeki cam sayısı test setimizde 4 adettir. Tamamı 4. tür olarak tahmin edilmiştir.\n",
    "+ Test setimizde bulunan 5. türdeki 3 adet camın hepsi yine 3. tür olarak tahmin edilmiştir.\n",
    "+ Test setimizde toplamda 19 adet bulunan 5. türdeki camın 1 tanesi 1. tür ve 1 tanesi 4. tür olarak yanlış tahmin edilmiştir. 17 adeti ise doğru tahmin edilerek 6. tür olarak belirlenmiştir.\n",
    "[Kaynak 29](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sınıflandırma metrikleri içinde yer alan $\\ accuracy\\_score $ kullanıldı.Bu metod tahmin edilen set ile ilgili y'lerin gerçek değerlerini karşılaştırarak hassasiyeti hesaplar.\n",
    "+ Parametre olarak test edilecek olan gerçek y değerlerinden oluşan dizi ile tahmin edilen y değerlerinden oluşan diziyi alır.\n",
    "+ $\\ normalize $ parametresi boolean bir değer alır. Varsayılan değeri True'dur. False değeri atanırsa doğru sınıflandırılmış örneklerin sayısını döndürür. True değeri atanırsa doğru sınıflandırılmış örneklerin oranını verir.\n",
    "+ Sonuç olarak eğer $\\ normalize==True $ olduğunda float bir değer arabilir. Bu durumda en iyi performans 1'dir. $\\ normalize==False $ olduğunda ise integer bir değer alır. Alınabilecek en iyi performans ise örnek örnek sayısıdır.\n",
    "[Kaynak 30](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6428571428571429"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Karar ağaçlarının veri setine uygulanması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=0, splitter='best')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier2=DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "classifier2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred2 = classifier2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier, bir veri kümesi üzerinde çok sınıflı sınıflandırma yapabilen bir sınıftır. Parametreleri ise şu şekildedir:\n",
    "+ $\\ criterion: $ Bölünme kalitesini ölçmek için kullanılır. Desteklenen kriterler Gini safsızlığı için 'gini' ve bilgi kazanımı için 'entropi' dir.\n",
    "+ $\\ criterion: $ Bölünme kalitesini ölçmek için kullanılır. Desteklenen kriterler Gini safsızlığı için 'gini' ve bilgi kazanımı için 'entropi' dir.\n",
    "+ $\\ max\\_depth: $ Ağacın maksimum derinliğidir. Eğer yoksa, düğümler tüm yapraklar saf olana kadar veya tüm yapraklar $\\ min\\_samples\\_split $ örneğinden daha az olana kadar genişletilir.\n",
    "+ $\\ min\\_samples\\_split: $Bir iç düğümün bölünmesi için gereken minimum örnek sayısıdır.Eğer int ise, minimum sayı olarak      $\\ min\\_samples\\_split $'i düşünülür.Float ise, o zaman $\\ min\\_samples\\_split $ bir yüzdedir ve ceil (min_samples_split * n_samples), her bölme için minimum numune sayısıdır.\n",
    "+ $\\ random\\_state: $Eğer int ise, random_state rastgele sayı üretecinin kullandığı tohumdur; \\n\",\n",
    "\"RandomState örneği, $\\ random\\_state $ rastgele sayı üreteci; None ise, rasgele sayı üreteci np.random tarafından kullanılan RandomState örneğidir.\n",
    "[Kaynak](http://scikit-learn.org/stable/modules/tree.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm2=confusion_matrix(y_test,y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36  6  5  1  0  0]\n",
      " [ 9 34  1  2  5  1]\n",
      " [14  0  0  0  0  0]\n",
      " [ 0  0  0  4  0  0]\n",
      " [ 0  0  0  0  3  0]\n",
      " [ 1  0  0  2  0 16]]\n"
     ]
    }
   ],
   "source": [
    "print(cm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Çıktı olarak elde edilen matris aşağıdaki gibi yorumlanabilir:   \n",
    "+ Test setinde 1. türde olan 14 cam vardır. Ağaç bunların 11'ini 1. tür, 3'ünü  2. tür olarak tahmin etmiştir.,   \n",
    "+ Elimizdeki test setinde 2. türden olan toplam 27 örneğin 14'ü doğru tahmin edilmiştir. Örneklerin 7'si 1. tür, 3 ,4 ve 5. türden 1'er adet olarak yanlış tahmin edilmiştir.   \n",
    "+ 3.türden olan 5 cam örneğinin 2'si 1. tür,  2'si 2. türdür. 1 örnek doğru tahmin edilmiştir.   \n",
    "+ 4.türdeki cam sayısı için test setimizde 2 adet cam vardır ve 1'i 2.tür diğeri 4. tür olarak tahmin edilmiştir. \n",
    "+ 5.türdeki 2 adet camın 1'i 2. tür diğeri 5. tür olarak tahmin edilmiştir.   \n",
    "+ Toplamda 7 adet olan 5. türdeki camın 1 tanesi 4.  tür, 6  tanesi 6. tür olarak tahmin edilmiştir. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66428571428571426"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHphJREFUeJzt3X2cVHXd//HXmxtFhOTOC1HMJe/SUhDw5gIxE8usRL1I\nkN8vLzSL+qlopY+yrEDNsqsbM7UU82YtXcS7RCpKCe/yBsFWURBNfyCr3CsIIYLwuf44h21Yd5nZ\n3Zmd3cP7+XjMY2bO+Z5zPufM7nvOfOfMOYoIzMys7WtX7gLMzKw4HOhmZhnhQDczywgHuplZRjjQ\nzcwywoFuZpYRDvQMk7SnpNMkdZb0X5IOLndNpSTpa5K6SDpc0mdzhg+TtKCA6SdK+n1pqyyMpOsl\nfb/cdVjb4kBvxSQtlPSupHWS3pb0R0l7N2IWq4AzgNeBbwDLS1JoM6VBuknS2vT2sqRrJfVp5Kz2\nBF4FbgNWbx0YEY9FxIHFrLk5JN0q6Yd1hlVICkkdACLiaxFxeQHzWijp+FLVam2LA731OykiugB9\ngGXANYVOGBHvRcSIiOgVEcMiYmXJqmy+OyOiK9ADOBXYA5jTmFCPiB9ERO+IOCginihVoTuKrW8u\n1nY40NuIiNgA3A3UdptI+pykf0h6R9JiSRNzp5H035IWSVol6fu5e3O53Qs5e4dnpfN5O+2+OFzS\n85JWS7o2Z777SXpE0hpJKyXdmTMu0vF7pp8stt7WS8r7s+SI2BQRLwKjgRXAhTnz/ryk6rSeJyQd\nmjNuYLot1kq6S9KdW/eCJR0rqSan7bclvZG2XSBpeN06JHWUVCXpHkk7SWon6WJJr6bbc4qkHnW2\n31hJr6fb5JJ867o9uXvxknpJmpau91uSHkvr+R3wYeCBdBt/K20/QtKLafuHJR3UmO2Ubp+lwC2S\nuqfLXpH+XUyT1Ddnfg9L+mH6eqyT9ICknpJuT/8un5FUkdM+JO3XnG1jDXOgtxGSOpOE3FM5g/8F\n/DfQDfgc8P8knZK2Pxj4NfB/SfbudwP2yrOYI4H90+X8ErgEOB74GDBK0ifSdpcDfwW6A32p51ND\nRLwZEV223oD7gMmFrm9EbAbuB4al63MYcDPwVaAncAMwVdLOknZK538ryR5+Fcle/gdIOhA4Dzg8\n/URwArCwTptdgD8A7wGjImIjMB44BfgESdfO28B1dWZ/NHAgMBz4QW6QNtOFQA2wO9Ab+C4QEbG1\nO+2kdDv/j6QDSNb/62n7P5EE/k4Fbqc90nH7AONIMuKW9PmHgXeBa+tMczpJ195ewL7Ak+k0PYD5\nwISibAXLy4He+v1B0mpgDfAp4KdbR0TEwxExNyK2RMTzJP+gW0P3C8ADEfF4Gkg/APLtIV8eERsi\n4q8kbxZVEbE8It4AHgMOS9ttIvkH3zNt//j2Zirp28BHgS81Yr0B3iQJBUjC5YaIeDoiNkdEJUng\nHpXeOgK/Svfw7wWeaWCem4GdgYMldYyIhRHxas74DwHTSfriz0rfWAC+BlwSETUR8R4wEfiCtu2W\nuDQi3o2I54DngP7bWbeL0j3o1enr+/x22m4ieVPeJ12/x6LhkzCNBv4YEQ9GxCbgZ8AuwBCS7dSB\nbbfTrDrTbwEmpN1170bEqoi4JyLWR8Ra4Ar+/Te21S0R8WpErAH+DLwaEQ9FxPvAXfz778ZKzIHe\n+p0SEd2ATiR7lo9I2gNA0pGSZqYfh9eQhE6vdLo9gcVbZxIR60m+JN2eZTmP363neZf08bcAAbPS\nj/YNBrWkE4EL0vV4N8/y69oLeCt9vA9wYZ0Q3JtkPfcEauqE3Ov1zTAi/kmy9zoRWC5psqQ9c5oc\nBRwKXFlnfvsA9+Usez7Jm0PvnDZLcx6v59/bqz4/i4huW2/pMhvyU+CfwF8lvSbp4u203RNYtPVJ\nRGwh+TvYKx33Rp31Wrzt5KxIu/eA5JOhpBuUdN29AzwKdJPUPmeaQv9urMQc6G1Euld6L0mIHJ0O\nvgOYCuwdEbsB15MELcASku4QoLYboWeRalkaEV+JiD1JukB+XV+/aNq9UUnSbVE3OLZLUjvgJJJP\nBpAEzxW5IRgRnSOiimRd95KknFk0eDRQRNwREUeThHQAP8kZ/Vfgx8AMSblhvRg4sc7yO6WfXkoq\nItZGxIUR8RFgBPDNnH7/unvqb5KsFwDpNtkbeIPCtlPd+V1I0o10ZER8CDhm66ybuj5WOg70NkKJ\nk0n6reeng7sCb0XEBklHAP8nZ5K7gZMkHZV2C0ygSK+3kmPbt75ZvE0SAlvqtPkQSR/4Jfm6ZOpM\n1yHte64i6c/9RTrqRuBr6acSSdpVyZfCXUn6bDcD56ZfFo4g+T6gvvkfKOk4STsDG0j2ILepPSL+\nh+TNcoakrZ94rgeukLRPOp/d09ej5JR8GbxfGsRrSNZ1a83LgI/kNJ8CfE7ScEkdSQL5PeAJ/r2d\nzku388nAEXkW35VkG61W8iWw+8NbMQd66/eApHXAOyT9l2PTo0AAzgEuk7SWpI98ytaJ0jbj02FL\nSLoAlpL8czfX4cDTaV1TgQsi4rU6bQaS7NldpZyjXbYzz9Hp+DXpPFcBgyLizXR9ZgNfIflC7m2S\nLogz03Ebgf8Cvkxy/PkZwAMNrOvOwJXp/N8D/gP4Tt1G6THgfwAeSoPs6rSuv6bb+ykaeNMogf2B\nh4B1JKH864iYmY77MfC9tCvooohYAHyR5IvqlSSfck6KiI052+lsku30RWAa2/+b+CVJH/xKknWe\nXuyVs+KRL3CxY5DUheSfeP+I+P/lrqfUJD1F8iXqLdtpMxU4MyLeaqhN1kl6Grh+e9vJ2g7voWeY\npJPSL7V2JTnaYS51DtHLCkmfkLRH2pUwluQIk3r3JnMO4VtD8klih1HPdjoU73VnhgM9204m+ZLs\nTZKP7adv53C3tu5AkkMFV5P0G38hIpY00HYPki6Eo9JpdiSN2U7WxrjLxcwsI7yHbmaWES168p1e\nvXpFRUVFSy7SzKzNmzNnzsqI2D1fuxYN9IqKCmbPnt2SizQza/MkLcrfyl0uZmaZ4UA3M8sIB7qZ\nWUb4iiRWNJs2baKmpoYNGzbkb2xF0alTJ/r27UvHjh3LXYq1Ag50K5qamhq6du1KRUUF257Qz0oh\nIli1ahU1NTX069ev3OVYK+AuFyuaDRs20LNnT4d5C5FEz549/YnIajnQragc5i3L29tyOdDNzDLC\nfehWMrq0uHuPMWH75x1atWoVw4cnF/JZunQp7du3Z/fdkx/XzZo1i5122qmg5dx888189rOfZY89\n9mhewWYtzIFumdGzZ0+qq6sBmDhxIl26dOGiiy5q9HxuvvlmBg4c6EC3NseBbjuEyspKrrvuOjZu\n3MiQIUO49tpr2bJlC2eddRbV1dVEBOPGjaN3795UV1czevRodtlll0bt2ZuVmwPdMu+FF17gvvvu\n44knnqBDhw6MGzeOyZMns++++7Jy5Urmzp0LwOrVq+nWrRvXXHMN1157LQMGDChz5WaNU9CXopK6\nSbpb0kuS5kv6T0k9JD0o6ZX0vnupizVrioceeohnnnmGwYMHM2DAAB555BFeffVV9ttvPxYsWMD5\n55/PX/7yF3bbbbdyl2rWLIUe5XI1MD0iPkpyaa/5wMXAjIjYH5iRPjdrdSKCL33pS1RXV1NdXc2C\nBQv4/ve/T8+ePXn++ecZNmwY1113HV/96lfLXapZs+QNdEm7AccAN0FyhfWIWE1yebPKtFklcEqp\nijRrjuOPP54pU6awcuVKIDka5vXXX2fFihVEBKeddhqXXXYZzz77LABdu3Zl7dq15SzZrEkK6UPv\nB6wAbpHUH5gDXAD0zrkW4VKgd30TSxoHjAP4cDKgmSUXmS/BVzL5DjNsKYcccggTJkzg+OOPZ8uW\nLXTs2JHrr7+e9u3bc/bZZxMRSOInP/kJAGeddRZf/vKX/aWotTl5rykqaTDwFDA0Ip6WdDXwDjA+\nIrrltHs7Irbbjz5YilZ3eQsHetHMnz+fgw46qNxl7HC83bNP0pyIGJyvXSF96DVATUQ8nT6/GxgI\nLJPUJ11YH2B5U4s1M7PmyxvoEbEUWCzpwHTQcGAeMBUYmw4bC9xfkgrNzKwghR6HPh64XdJOwGvA\nWSRvBlMknQ0sAkaVpkQzMytEQYEeEdVAff03w4tbjpmZNZXPtmhmlhEOdDOzjHCgW+lIxb0VoEuX\nLrWP//SnP3HAAQewaNGiUq3hNqqrq5HE9OnTSzL/IUOGlGS+lh0OdMukGTNmcP755/PnP/+ZffbZ\np6Bp3n///Wa1q6qq4uijj6aqqqrgOhvjiSeeKMl8LTsc6JY5jz76KF/5yleYNm0a++67LwAPPPAA\nRx55JIcddhjHH388y5YtA5Lzpp9xxhkMHTqUM844g4ULFzJs2DAGDhzIwIEDa0P04YcfZtiwYYwY\nMYKDDz74A8uMCO666y5uvfVWHnzwwW2u83n55Zdz4IEHcvTRRzNmzBh+9rOfAXDjjTdy+OGH079/\nf0aOHMn69esBWLZsGaeeeir9+/enf//+tTXkfvowq1dEtNhtUPK7zNZ1s6KZN2/etgPK8Fp16NAh\nunfvHs8999w2w996663YsmVLRETceOON8c1vfjMiIiZMmBADBw6M9evXR0TEv/71r3j33XcjIuLl\nl1+OQYMGRUTEzJkzo3PnzvHaa6/Vu9zHH388jjvuuIiIGDNmTNx9990RETFr1qzo379/vPvuu/HO\nO+/EfvvtFz/96U8jImLlypW1019yySXxq1/9KiIiRo0aFVdddVVERLz//vuxevXqiIjYdddd6132\nB7a7ZQ4wOwrIWJ8P3TKlY8eODBkyhJtuuomrr766dnhNTQ2jR49myZIlbNy4kX79+tWOGzFiBLvs\nsgsAmzZt4rzzzqO6upr27dvz8ssv17Y74ogjtpkuV1VVFaeffjoAp59+OrfddhsjR47k73//Oyef\nfDKdOnWiU6dOnHTSSbXTvPDCC3zve99j9erVrFu3jhNOOAGAv/3tb9x2220AtG/f3qf1tYK5y8Uy\npV27dkyZMoVZs2bxox/9qHb4+PHjOe+885g7dy433HDDNl0iu+66a+3jq666it69e/Pcc88xe/Zs\nNm7cWG+7XJs3b+aee+7hsssuo6KigvHjxzN9+vS8Z2w888wzufbaa5k7dy4TJkzYpiazpnCgW+Z0\n7tyZP/7xj9x+++3cdNNNAKxZs4a99toLSC5H15A1a9bQp08f2rVrx+9+9zs2b96cd3kzZszg0EMP\nZfHixSxcuJBFixYxcuRI7rvvPoYOHcoDDzzAhg0bWLduHdOmTaudbu3atfTp04dNmzZx++231w4f\nPnw4v/nNb4DkzWLNmjVN2g6243GgW+kUuxe9EXr06MH06dP54Q9/yNSpU5k4cSKnnXYagwYNolev\nXg1Od84551BZWUn//v156aWXGtwrz1VVVcWpp566zbCRI0dSVVXF4YcfzogRIzj00EM58cQTOeSQ\nQ2q7UC6//HKOPPJIhg4dykc/+tHaaa+++mpmzpzJIYccwqBBg5g3b16j1t12XHlPn1tMPn1utvk0\nrvVbt24dXbp0Yf369RxzzDFMmjSJgQMHFm3+3u7ZV+jpc1v0S9E5DEIUN9KDVnbBDLM6xo0bx7x5\n89iwYQNjx44tapib5fJRLmYldscdd5S7BNtBuA/dzCwjHOhmZhnhQDczywgHuplZRjjQrWTKcPZc\nAK644go+9rGPceihhzJgwAAuvfRSvvOd72zTprq6uvZQv4qKCoYNG7bN+AEDBvDxj3+82dvArCU5\n0C1TnnzySaZNm8azzz7L888/z0MPPcQnP/lJ7rzzzm3aTZ48mTFjxtQ+X7t2LYsXLwaS47rN2iIH\numXKkiVL6NWrFzvvvDMAvXr14phjjqF79+48/fTTte2mTJmyTaCPGjWqNvSrqqq2GWfWVjjQLVM+\n/elPs3jxYg444ADOOeccHnnkEQDGjBnD5MmTAXjqqafo0aMH+++/f+10I0eO5N577wWSc6fnnhXR\nrK3Y4X9YpEtb3y9NY4JPR9BUXbp0Yc6cOTz22GPMnDmT0aNHc+WVVzJ69GiGDBnCz3/+8w90twD0\n7NmT7t27M3nyZA466CA6d+5cpjUwa7odPtAte9q3b8+xxx7LscceyyGHHEJlZSVnnnkm/fr145FH\nHuGee+7hySef/MB0o0eP5txzz+XWW29t+aLNisCBbpmyYMEC2rVrV9udUl1dXXtN0TFjxvCNb3yD\nj3zkI/Tt2/cD05566qksWbKEE044gTfffLNF6zYrhoICXdJCYC2wGXg/IgZL6gHcCVQAC4FREfF2\nacpsYRNL0OUxsfV17ZRaOU5kuW7dOsaPH8/q1avp0KED++23H5MmTQLgtNNO4/zzz+eaa66pd9qu\nXbvy7W9/uyXLNSuqxuyhfzIiVuY8vxiYERFXSro4fe7/BiurQYMG1V5Uua5evXqxadOmDwxfuHDh\nB4ZVVFTwwgsvFLs8s5JqzlEuJwNbL/1SCZzS/HLMzKypCg30AB6SNEfSuHRY74hYkj5eCvQuenVm\nZlawQrtcjo6INyT9B/CgpJdyR0ZESKq3xzR9A0jfBD7cjFKtLYgI1Jjf6VuztOQVx6z1K2gPPSLe\nSO+XA/cBRwDLJPUBSO+XNzDtpIgYnFw+affiVG2tUqdOnVi1apVDpoVEBKtWraJTp07lLsVaibx7\n6JJ2BdpFxNr08aeBy4CpwFjgyvT+/lIWaq1f3759qampYcWKFeUuZYfRqVOneg/BtB1TIV0uvYH7\n0o/RHYA7ImK6pGeAKZLOBhYBo0pXprUFHTt2pF+/fuUuw2yHlTfQI+I1oH89w1cBw0tRlJmZNZ5P\nzmVmlhEOdDOzjHCgm5llhAPdzCwjHOhmZhnhQDczywgHuplZRjjQzcwywoFuZpYRDnQzs4xwoJuZ\nZYQD3cwsIxzoZmYZ4UA3M8sIB7qZWUY40M3MMsKBbmaWEQ50M7OMcKCbmWWEA93MLCMc6GZmGeFA\nNzPLCAe6mVlGONDNzDLCgW5mlhEFB7qk9pL+IWla+ryHpAclvZLedy9dmWZmlk9j9tAvAObnPL8Y\nmBER+wMz0udmZlYmBQW6pL7A54Df5gw+GahMH1cCpxS3NDMzawxFRP5G0t3Aj4GuwEUR8XlJqyOi\nWzpewNtbn9eZdhwwLnn24UGwqHjVA4GKOj8AkX+bNFaj6izgNTGzHYekORExOF+7vHvokj4PLI+I\nOQ21ieRdod4UiohJETE4KWb3fIszM7Mm6lBAm6HACEmfBToBH5L0e2CZpD4RsURSH2B5KQs1M7Pt\ny7uHHhHfiYi+EVEBnA78LSK+CEwFxqbNxgL3l6xKMzPLqznHoV8JfErSK8Dx6XMzMyuTQrpcakXE\nw8DD6eNVwPDil2RmZk3hX4qamWWEA93MLCMc6GZmGeFANzPLCAe6mVlGONDNzDLCgW5mlhEOdDOz\njHCgm5llhAPdzCwjHOhmZhnhQDczywgHuplZRjjQzcwywoFuZpYRDnQzs4xwoJuZZYQD3cwsIxzo\nZmYZ4UA3M8sIB7qZWUY40M3MMsKBbmaWEQ50M7OMyBvokjpJmiXpOUkvSro0Hd5D0oOSXknvu5e+\nXDMza0ghe+jvAcdFRH9gAPAZSUcBFwMzImJ/YEb63MzMyiRvoEdiXfq0Y3oL4GSgMh1eCZxSkgrN\nzKwgBfWhS2ovqRpYDjwYEU8DvSNiSdpkKdC7gWnHSZotaTasKErRZmb2QQUFekRsjogBQF/gCEkf\nrzM+SPba65t2UkQMjojBsHuzCzYzs/o16iiXiFgNzAQ+AyyT1AcgvV9e/PLMzKxQhRzlsrukbunj\nXYBPAS8BU4GxabOxwP2lKtLMzPLrUECbPkClpPYkbwBTImKapCeBKZLOBhYBo0pYp5mZ5ZE30CPi\neeCweoavAoaXoigzM2s8/1LUzCwjHOhmZhnhQDczywgHuplZRjjQzcwywoFuZpYRDnQzs4xwoJuZ\nZYQD3cwsIxzoZmYZ4UA3M8sIB7qZWUY40M3MMsKBbmaWEQ50M7OMcKCbmWWEA93MLCMc6GZmGeFA\nNzPLCAe6mVlGONDNzDLCgW5mlhEOdDOzjHCgm5llhAPdzCwj8ga6pL0lzZQ0T9KLki5Ih/eQ9KCk\nV9L77qUv18zMGlLIHvr7wIURcTBwFHCupIOBi4EZEbE/MCN9bmZmZZI30CNiSUQ8mz5eC8wH9gJO\nBirTZpXAKaUq0szM8mtUH7qkCuAw4Gmgd0QsSUctBXo3MM04SbMlzYYVzSjVzMy2p+BAl9QFuAf4\nekS8kzsuIgKI+qaLiEkRMTgiBsPuzSrWzMwaVlCgS+pIEua3R8S96eBlkvqk4/sAy0tTopmZFaJD\nvgaSBNwEzI+IX+SMmgqMBa5M7+8vSYVmZi1Il6rcJTRZ3kAHhgJnAHMlVafDvksS5FMknQ0sAkaV\npkQzMytE3kCPiMeBht6yhhe3HDMzayr/UtTMLCMc6GZmGeFANzPLCAe6mVlGFHKUi5nZDiMmNm96\n1f8by2Yq7FBK76GbmWWEA93MLCMc6GZmGeFANzPLCAe6mVlGONDNzDLCgW5mlhEOdDOzjHCgm5ll\nhH8pagVrjSf+jwml+FWeWdvkPXQzs4xwoJuZZYQD3cwsIxzoZmYZ4UA3M8sIB7qZWUY40M3MMsKB\nbmaWEQ50M7OMyBvokm6WtFzSCznDekh6UNIr6X330pZpZgBS8W+WHYXsod8KfKbOsIuBGRGxPzAj\nfW5mZmWUN9Aj4lHgrTqDTwYq08eVwClFrsvMzBqpqX3ovSNiSfp4KdC7oYaSxkmaLWk2rGji4szM\nLJ9mfykaEQE0eMq7iJgUEYMjYjDs3tzFmZlZA5oa6Msk9QFI75cXryQzM2uKpgb6VGBs+ngscH9x\nyjEzs6bKe4ELSVXAsUAvSTXABOBKYIqks4FFwKhSFmlmJVSuYxfDFycptryBHhFjGhg1vMi1mJlZ\nM/iXomZmGeFrippZefhnqkXnPXQzs4xwoJuZZYQD3cwsIxzoZmYZ4UA3M8sIB7qZWUYoWvDXWtLg\ngNlFnWdQ/EOf1PC5xpqsUXW21l/QtcbDzFrrtiqRUrwEpfgfKoWy/18WqBR1guYkJzjcPu+hm5ll\nhAPdzCwjHOhmZhnhQDczywgHuplZRjjQzcwywoFuZpYRDnQzs4xwoJuZZYQD3cwsIxzoZmYZ4UA3\nM8sIB7qZWUY40M3MMsKBbmaWEQ50M7OMaFagS/qMpAWS/inp4mIVZWZmjdfkQJfUHrgOOBE4GBgj\n6eBiFWZmZo3TnD30I4B/RsRrEbERmAycXJyyzMyssTo0Y9q9gMU5z2uAI+s2kjQOGJc+fQ/0QjOW\n+QGluRpik+faC1jZ7Dm2xmt3Jhpcv7Ip3rZqfetWXMX52yyr7VbapNevleXH9hxYSKPmBHpBImIS\nMAlA0uxCLnTaVnn92q4srxt4/do6SbMLadecLpc3gL1znvdNh5mZWRk0J9CfAfaX1E/STsDpwNTi\nlGVmZo3V5C6XiHhf0nnAX4D2wM0R8WKeySY1dXlthNev7cryuoHXr60raP0UEaUuxMzMWoB/KWpm\nlhEOdDOzjGiRQJd0s6TlUnGPQW8NJO0taaakeZJelHRBuWsqJkmdJM2S9Fy6fpeWu6ZSkNRe0j8k\nTSt3LcUmaaGkuZKqCz38rS2R1E3S3ZJekjRf0n+Wu6ZikHRg+pptvb0j6evbnaYl+tAlHQOsA26L\niI+XfIEtSFIfoE9EPCupKzAHOCUi5pW5tKKQJGDXiFgnqSPwOHBBRDxV5tKKStI3gcHAhyLi8+Wu\np5gkLQQGR0QmfzglqRJ4LCJ+mx5x1zkiVpe7rmJKT7XyBnBkRCxqqF2L7KFHxKPAWy2xrJYWEUsi\n4tn08VpgPsmvaDMhEuvSpx3TW6a+SZfUF/gc8Nty12KNI2k34BjgJoCI2Ji1ME8NB17dXpiD+9CL\nSlIFcBjwdHkrKa60O6IaWA48GBGZWj/gl8C3gC3lLqREAnhI0pz0VBxZ0g9YAdySdpn9VtKu5S6q\nBE4HqvI1cqAXiaQuwD3A1yPinXLXU0wRsTkiBpD8GvgISZnpNpP0eWB5RMwpdy0ldHT6+p0InJt2\ngWZFB2Ag8JuIOAz4F5CpU3mn3UgjgLvytXWgF0Hat3wPcHtE3Fvuekol/Sg7E/hMuWspoqHAiLSf\neTJwnKTfl7ek4oqIN9L75cB9JGdKzYoaoCbnU+PdJAGfJScCz0bEsnwNHejNlH5peBMwPyJ+Ue56\nik3S7pK6pY93AT4FvFTeqoonIr4TEX0jooLkY+3fIuKLZS6raCTtmn5ZT9oV8WkgM0ebRcRSYLGk\nrWcjHA5k4oCEHGMooLsFWuBsiwCSqoBjgV6SaoAJEXFTSyy7BQwFzgDmpv3MAN+NiD+VsaZi6gNU\npt+ytwOmRETmDu3LsN7Afcl+Bx2AOyJienlLKrrxwO1p18RrwFllrqdo0jfhTwFfLai9f/pvZpYN\n7nIxM8sIB7qZWUY40M3MMsKBbmaWEQ50M7OMcKCbmWWEA93MLCP+F46+fMOG+zm6AAAAAElFTkSu\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2049c4784e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.ylim((0,65))\n",
    "plt.xlim((1,7))\n",
    "plt.hist(y_test,bins='auto',color='green',label='Test')\n",
    "plt.hist(y_pred2, bins='auto', color='red',label='Karar Agaci')\n",
    "plt.hist(y_pred, bins='auto',color='blue',label='SVM')\n",
    "plt.legend(loc='upper center')\n",
    "plt.title(\"Bağımsız Değişken Histogramı\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'No free lunch' teoremine göre tüm alanlarda en iyi performansı gösteren tek bir öğrenme algoritması yoktur.[Kaynak 31](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3099263/#R50). Bu nedenle, bir dizi sınıflandırıcı test edilmelidir. Cam Tanılama veri setine SVM ve karar ağaçları algoritmaları uygulanarak alınan sonuçları karşılaştırdık. En iyi performansı test setinin veri setine oranı 0.65 olduğunda elde ettik. işlemler sonucunda SVM algoritması 0.6428571428571429'luk doğruluk puanı elde ederken, karar ağaçları algoritması 0.6642857142857146'lık doğruluk puanı elde etti. Birbirine oldukça yakın olan bu sonucun test seti boyutu değiştikçe değişimi ise şu şekilde olmuştur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "|  Test_size        | SVM                | Karar Ağacı Algoritması |\n",
    "| ----------------- |:------------------:|:-----------------------:|\n",
    "| 0.1               | 0.54545454545454541|0.63636363636363635      |\n",
    "| 0.2               | 0.53488372093023251|0.63636363636363635      | \n",
    "| 0.3               | 0.59999999999999998|0.63076923076923075      | \n",
    "| 0.4               | 0.60465116279069764|0.60465116279069764      | \n",
    "| 0.5               | 0.62616822429906538|0.59813084112149528      | \n",
    "| 0.6               | 0.63565891472868219|0.65116279069767447      | \n",
    "| 0.65              | 0.6428571428571429 |0.6642857142857146       | \n",
    "| 0.7               | 0.6333333333333333 |0.65333333333333332      | \n",
    "| 0.8               | 0.5                |0.65697674418604646      | \n",
    "| 0.9               | 0.44559585492227977|0.56994818652849744      | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
