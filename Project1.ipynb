{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "\n",
    "## Members\n",
    "\n",
    "1. First member: Berna Soysal soysalb@itu.edu.tr\n",
    "2. Second member: Hatice Aslan aslanhat@itu.edu.tr\n",
        "\n",
    "## Description of the project\n",
    "\n",
    "### The methods to be used\n",
    "\n",
    "#### SVM\n",
    "SVM (Support Vector Machines-Destek Vektör Makineleri) verileri sınıflandırma ve regresyon analizinde kullanılan, öngörülen ve gerçek değer arasındaki farkı en aza indirmeyi hedefleyen denetlenen öğrenme modellerinden biridir.[Kaynak 5](Girosi, 1998) N-boyutlu uzayda her bir veri her özelliği bir değere sahip olacak şekilde bir koordinat ile belirlenir.Sınıflandırma için sınıfları ayıracak bir hiperdüzlem veya hiperdüzlem kümesi bulunur.[Kaynak 6](http://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation) Bulunan hiperdüzlem sayesinde verilerin tamamı lineer bağımsız veya bir çoğu lineer bağımsız hale gelir. SVM iki farklı sınıf arasındaki bu sınırın nasıl çizilmesi gerektiğini verilen bağımlı ve bağımsız değişkenler arasındaki ilişkiyi öngörmek için kullanılan veri olarak tanımlayabileceğimiz eğitim seti üzerinden eğitim algoritması yoluyla belirler.  \n",
    "\n",
    "[Kaynak 1](https://en.wikipedia.org/wiki/Support_vector_machine#Parameter_selection) ve  \n",
    "[Kaynak 2](https://www.analyticsvidhya.com/blog/2015/10/understaing-support-vector-machine-example-code/)\n",
    "\n",
    "Eğitim örneği formu\n",
    "\n",
    " + $\\{x_i, y_i\\}$, $i=1,...,n$ ve $x_i \\in \\mathbb{R}^d$ ve $y_i \\in \\{−1, +1\\}$\n",
    " + $\\{x_i\\}$: eşdeğişkenler veya girdi vektörü\n",
    " + $\\{y_i\\}$: bağımlı değişkenler\n",
    "\n",
    "\n",
    "Doğrusal ayrılabilen bir veri setini düşünürsek $\\ f(x)=w^T x_i − b $ olacak şekilde düz bir çizgi ile verileri sınıflandırabiliriz. Rastgele seçilen bir verinin  $\\ y_i=-1 $ değerini alması ikiye ayrılan bölgenin hangi tarafında olduğunu gösterir ve bu durumda $\\ f(x_i)<0 $ olur. Diğer taraftan $\\ y_i=1 $ değeri ise verinin diğer bölgede yer aldığını ve $\\ f(x_i)>0 $ olduğunu gösterir.Böylelikle elde edeceğimiz test durumlarını $\\ y_{test}=sign(x_{test}) $ kuralına göre sınıflandırabiliriz. \n",
    "\n",
    "Bu yol ile çizilebilecek hiperdüzlemlerin sayısı sonsuzdur. Seçilen hiperdüzlemin verdiği sonuçlar test durumlarında farklı performans gösterebilir. Belirli bir gruba yakın seçilecek hiperdüzlem diğer grup elemanlarının sınıflandırılmasında hataya neden olur. Bu neden seçilecek hiperdüzlem iki sınıfın eğitim setlerine eşit uzaklıkta olacak şekilde belirlenmelidir.\n",
    "\n",
    "Geometrik olarak, ayrımı sağlayan fonksiyonu $\\ w^T x = b $ olarak belirlersek $\\ b=0 $ olduğunda elde edilen $\\ x $ 'in alacağı her değerin $\\ w $ vektörüne dik olduğu görülür. Bu durumda belirlenen hiperdüzlemin sınıflandırma yapması beklenemez. Hiperdüzlemi orijinin $\\ a $ vektörü kadar uzağına taşırsak kullanılacak fonksiyon $\\ (x-a)^T w = 0 $  olacak şekilde değişir. \n",
    "$\\ b = a^T w $ buradan $\\ b $ 'nin, $\\ a $ 'nın vektör w üzerindeki görüntüsü olduğu çıkarımı yapılabilir. $\\ a $'yı düzleme dik olarak seçtiğimizde $\\ ||a||  = |b|  /  ||w|| $ uzunluğu hiperdüzlem ile orijin arasındaki en kısa uzaklıktır. \n",
    "\n",
    "Sınıflandırma için kullandığımız hiperdüzleme paralel, hiperdüzleme en yakın eğitim örneklerini her iki tarafta kesecek şekilde iki hiperdüzlem seçilir. Bu iki düzleme destek hiperdüzlemleri adı verilir. Hiperdüzlem ve destek hiperdüzlemleri arasındaki uzaklığı $\\ d_+ $ ve $\\ d_- $ olarak tanımlayalım. Hiperdüzlemler arasındaki uzaklık- marjin, $\\ \\gamma  $,  $\\ d_+ + d_- $ olarak bulunur. Uzaklığı maksimum yapacak şekilde, destek hiperdüzlemlerine eşit uzaklıkta bir ayırıcı hiperdüzlem bulmalıyız. Ayırıcı hiperdüzlemin denklenmini $\\ w^T x = b $ seçmiştik. Buradan yola çıkarak destek hiperdüzlemlerini $$ w^T x = b + \\delta $$\n",
    "$$ w^T x = b - \\delta $$ olarak tanımlayabiliriz.\n",
    "\n",
    "Elde etiğimiz eşitliklerde $\\ w $,$\\ b $ ve $\\ \\delta $ 'nın sabit bir $\\ \\alpha $ ile çarpılması $\\ x $ 'in eşitlikleri sağlamasını engellemez. Bu nedenle değişken sayısını azaltmak ve belirsizliği ortadan kaldırmak için $\\ \\delta = 1 $ diyebiliriz. Buradan, $$ d_+ = \\frac {|| b+1 | - | b ||}{|| w ||} ,  b \\notin \\{-1,0\\} $$ elde edilir. Orijinin hiperdüzlemler arasında kaldığı durum için ise $$ d_+ = \\frac {|| b+1 | + | b ||}{|| w ||} , b \\in \\{-1,0\\} $$ elde edilir. Bu yolla marjini, $\\ \\delta = \\frac {2}{||w||}$ olarak buluruz.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Bu eşitlikler göz önüne alındığında çizelecek hiperdüzlemler aşağıdaki çözümleri sağlamalıdır.\n",
    " \n",
    " İlk durum kullanılarak $\\ w^T x_i − b ≤ −1,  ∀ y_i = −1 $ çözümü elde edilir.\n",
    " \n",
    " İkinci durumun gerçeklenmesi ise $\\ w^T x_i − b ≥ +1,  ∀ y_i = +1 $ ile sağlanabilir.\n",
    " \n",
    " Birinci ve ikinci çözümlerin genelleştirilmesiyle elde edilen $\\ y_i (w^T x_i − b) − 1 ≥ 0 $    çözümü hiperdüzlemler için sağlanması gereken genel çözümdür.\n",
    " \n",
    " Böylece SVM'in primer problemini  \n",
    " \n",
    " $$ minimize_{w,b}   \\frac{1}{2} ||w||^2 $$ ve  $\\ subject $ $\\ to $ $\\ y_i (w^T x_i − b) − 1 ≥ 0 $ $\\ \\forall i $ \n",
    " \n",
    " \n",
    " \n",
    " şeklinde formülize edebiliriz. Bu yolla destek hiperdüzlemlerinin farklı taraflarında kalan eğitim durumlarına göre belirlenen kısıtlar yolu ile marjı maksimize edebiliriz. Hiperdüzlemleri destekleyen, sorunun çözümünün bulunmasını sağlayan hiperdüzlemlerin üzerindeki veri durumlarına destek vektörü denir.\n",
    " \n",
    "Kernelize etmeyi algoritma inputlarının verimliliğini artırmak için daha küçük input ile değiştirildiği daha verimli algoritma tekniği olarak tanımlayabiliriz.Primer problem çeşitli programlarla çözülebilmesine rağmen lineer olarak sadece veri vektörleri arasındaki iç çarpıma bağlı olmadığından kernelize edilmek için uygun değildir.  Bu nedenle elde ettiğimiz problemi dual forma dönüştürmeliyiz.\n",
    "$$ \\mathcal{L}(w,b, \\alpha ) = \\frac{1}{2} ||w||^2 - \\sum_{i=1}^N \\alpha_i [y_i (w^T x_i − b) − 1] $$\n",
    "\n",
    "$\\ min_w min_\\alpha \\mathcal{L}(w,\\alpha) $ kısıtları primer problemin minimum değerdeki çözümünü elde ederken kullanılır. Orijinal çözümdeki fonksiyon konveks olduğundan minimizasyon ve maksimizasyon durumlarını değiştirmeliyiz. Ancak eyer noktasını sağlayan yeni kısıtlar oluşturulmalıdır. Dual problemin $\\ w $ ve$\\ b $'ye göre türevi alındığında\n",
    "$$ w - \\sum_i \\alpha_i y_i x_i  \\Rightarrow  w^* = \\sum_i \\alpha_i y_i x_i   $$\n",
    "$$ \\sum_i \\alpha_i y_i = 0 $$\n",
    "\n",
    "\n",
    "Elde ettiklerimizi Lagrange denkleminde yerine koyarsak $$ maximize \\quad  \\mathcal{L}_D = \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{ij} \\alpha_i \\alpha_j y_iy_j x_i^Tx_j$$\n",
    "            $$ subject \\quad to \\quad \\sum_i \\alpha_iy_i =0 $$ $$ \\alpha_i \\geq 0, \\quad  \\forall i $$\n",
    "            \n",
    "Burada değişken sayısı olan $\\ \\alpha_i $, veri durumu sayısı olan $\\ N $'e eşittir. \n",
    "\n",
    "Problem sadece $\\ x_ix_j $ iç çarpımına bağımlıdır. $\\ x_i^Tx_j $ çarpımı $\\ k(x_i,x_j)$  şeklinde dual problem kernelize edilebilir.\n",
    "            \n",
    "Dualite teorisi konveks problemleri konkav hale getirerek dual problemle birbirine eş primer problem sonuçları elde edilmesini sağlar. Yani $\\ \\mathcal{L}_P(w^*)= \\mathcal{L}_D(\\alpha^*) $'dır.\n",
    "\n",
    "Eyer noktasının sağlayan yeni kısıtlar ve problemin çözümünde Karush-Kuhn-Tucker(KKT) koşullarını kullanılmalıdır. Bu koşullar genel olarak gerekli ve konveks optimizasyon problemleri için yeterlidir.\n",
    "\n",
    "Primer problemin $\\ w $ ve $\\ b $'ye göre türevi alınarak\n",
    "\n",
    "$$ ∂_bL_P = 0 \\rightarrow w − \\sum_i α_iy_ix_i = 0 $$\n",
    "$$ ∂_bL_P = 0 \\rightarrow  \\sum_i α_iy_i = 0 $$\n",
    "elde edilir.\n",
    "Primer problemin kısıtı olan $\\ kısıt-1 $ $$ y_i (w^T x_i − b) − 1 ≥ 0 $$\n",
    "Lagrange çarpanlarının negatif olmadığı eşitsizlik $\\ çarpan $$\\ koşulu $ için $$ \\alpha_i \\geq 0 $$ \n",
    "ve tümler gevşeklik koşulu $$ \\alpha_i [y_i (w^T x_i − b) − 1]=0 $$ sağlanmalıdır.\n",
    "\n",
    "Tümler gevşeklik koşulu, $\\ y_i (w^T x_i − b) − 1 > 0 $ iken $\\ \\alpha_i=0 $ olduğu veya $\\ \\alpha_i \\geq 0 $ iken $\\ y_i (w^T x_i − b) − 1 = 0 $ olduğu durumlarda sağlanır. Burada eşitsizlikler$\\ \\textit{aktif} $ kısıtlar, diğerleri ise aktif olmayan kısıtlardır. Çözüm bulunurken sonucu değiştirmediğinden aktif olmayan kısıtları kaldırabiliriz. \n",
    "\n",
    "Eğitim grubundaki destek vektörleri olan $\\ \\alpha_i > 0 $, destek hiperdüzlemleri üzerinde aktif kısıtlardır ve çözümü sağlarlar. Ancak bir çoğu hiperdüzlemleri sıfırlarlar. Bu da seyrek olmalarına neden olur.\n",
    "$$ f(x) = w^{*T}x - b^* = \\sum_i \\alpha_i y_i x_i^Tx - b^* $$\n",
    "fonksiyonu gelecekteki test durumlarını sınıflandırmak için kullanılacaktır.\n",
    "\n",
    "KKT koşullarından tümler gevşeklik koşulunu kullanarak $$ b^* = ( \\sum_j \\alpha_j y_j x_j^T x_i - y_i) $$  olarak bulunur. Burada her $\\ i $ bir destek vektörünü simgeler ve $\\ y^2=1 $ . Bu yolla herhangi bir destek vektörünü kullanarak $\\ b $'yi bulabiliriz ancak sayısal anlamda dengeyi sağlamak için ortalamalarını almak tercih edilebilir.\n",
    "\n",
    "Ulaşılan en önemli sonuç $\\ f(x) $ fonksiyonunun $\\ x^T_i x_i $ ile ifade edilebileceğidir; bu da, kernel matrisleri $\\ k(x_ix_j) $ ile yer değiştirebilir ve yüksek boyutlu doğrusal olmayan uzaylarda da kullanılabilir olduğunu gösterir. Ayrıca $\\ \\alpha $ seyrek olduğundan yeni input $\\ x $'leri tahmin etmek için fazla sayıda kernel değerlendirmeye gerek kalmamıştır.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[Kaynak 3: A First Encounter with Machine Learning Max Welling Donald Bren School of Information and Computer Science University of California Irvine April 21, 2010](https://www.ics.uci.edu/~welling/teaching/273ASpring10/IntroMLBook.pdf)\n",
    "\n",
    "\n",
    "\n",
    "SVM bir çok problemi başarıyla çözebilir:\n",
    "+ Text ve hypertext kategorizasyonu\n",
    "+ Email filtreleme\n",
    "+ Belgeleri konusuna göre sınıflandırma\n",
    "+ Web aramaları\n",
    "+ İmage sınıflandırması\n",
    "+ Protein sınıflandırması\n",
    "+ Kanser sınıflandırması\n",
    "+ El yazısı tanımlama\n",
    "[Kaynak 4](www.cs.utexas.edu/users/mooney/cs391L/svm.ppt)\n",
    "\n",
    "Scikit-learn Python diliyle çalışan açık kaynaklı bir makine öğrenmesi kütüphanesidir. Veri analizi ve veri madenciliği için verimli bir araç olan sckit-learn; NumPy, SciPy ve matplotlib sayısal ve bilimsel kütüphaneleri ile birlikte çalışmak üzere tasarlanmıştır. [Kaynak 7](http://scikit-learn.org/stable/index.html)\n",
    "\n",
    "\n",
    "Scikit-learn'de çoklu sınıflandırma SVC, NuSVC ve LinearSVC sınıfları ile yapılabilir. SVC ve NuSVC benzer metodlardır ancak parametre setleri birbirinden biraz farklıdır ve matematiksel formülasyonları farklıdır. Diğer taraftan LinearSVC destek vektör makineleri için kullanılan bir diğer metoddur. LinearSVC doğrusal kernel ile çalıştığından kernel parametresi kullanılmaz.  \n",
    "\n",
    "  $\\{x_i, y_i\\}$, $i=1,...,n$ ve $x_i \\in \\mathbb{R}^d$ ve $y_i \\in \\{−1, +1\\}$ iken burada $\\ x_i $ eğitim vektörleri ve $\\ y_i $ bağımlı değişkenlerdir. SVC primal problemi \n",
    "  $$ min_{w,b,\\zeta} \\quad \\frac{1}{2}w^Tw + C \\sum_{i=1} \\zeta_i $$\n",
    "  $$ subject \\quad to \\quad y_i(w^T \\phi(x_i) + b) \\geq 1-\\zeta_i $$\n",
    "  $$ \\zeta_i \\geq 0, i=1,2,..,n $$ olarak tanımlar.\n",
    "\n",
    "Dual problem ise $$ min_a \\frac{1}{2} \\alpha^T Q \\alpha - e^T \\alpha $$\n",
    "$$ subject \\quad to \\quad y^T \\alpha = 0 $$\n",
    "$$   \\qquad 0 \\leq \\alpha_i \\leq C, \\quad i=1,2,...,n $$\n",
    "burada $\\ e $ bütün elemanları 1 olan vektör, $\\ C >0 $ üst sınır, $\\ Q $ $\\ nxn $ simetrik matris, $\\ Q_{ij} \\equiv y_iy_j K(x_i,x_j) $'dir. Kernel $\\ K(x_ix_j) = \\phi(x_i)^T \\phi(x_j) $. Böylece $\\ \\phi $ fonksiyonu yardımıyla eğitim vektörleri daha yüksek boyutlu uzayda incelenir.\n",
    "\n",
    "Elde edilen gözlemlere dayanarak istatiksel kurallar uygulanarak $\\ sgn( \\sum_{i=1}^n y_i \\alpha_i K(x_i,x) + \\rho ) $ elde edilmiştir. libsvm ve liblinear kütüphaneleri SVM modellerinde kullanılırken $\\ C $ parametresi kullanılırken, diğer kütüphaneler için $\\ \\alpha $ kullanılır. $\\ C $ ile $\\ \\alpha $ arasındaki ilişki $\\ C = \\frac {n_örnek}{\\alpha} $ şeklindedir.\n",
    "$\\ y_i \\alpha_i $ çarpımına dual_coef_ , $\\ \\rho $ bağımsız değişkenine intercept_ parametreleri yoluyla ulaşılabilir. \n",
    "[Kaynak 8](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.7215)\n",
    "[Kaynak 9](https://link.springer.com/article/10.1007%2FBF00994018)\n",
    "[Kaynak 10](http://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation)\n",
    "\n",
    "\n",
    "NuSVC metodu ile destek vektör sayısını ve eğitim hatasını kontrol eden$\\ v $ parametresi kullanılmaya başlanmıştır. Eğitim hatası, eğitimli modeli eğitim verilerine uyguladığımızda elde edilen hatadır. $\\ v \\in (0,1] $ parametresi eğitim hataları oranı için üst sınır, destek vektör oranı için alt sınırdır.\n",
    "$\\ v $-SVC formülasyonu $\\ C $-SVC'nin yeniden parametrize edilmiş şeklidir dolayısıyla matematiksel olarak eşitidir.\n",
    "[Kaynak 10](http://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation)\n",
    "\n",
    "\n",
    "SVR ise  $x_i \\in \\mathbb{R}^d$ test vektörleri ve $y_i \\in \\mathbb{R}^n$ vektörüüerinde tanımlıdır ve primal problemi\n",
    " $$ min_{w,b,\\zeta,\\zeta^*} \\quad \\frac{1}{2}w^Tw + C \\sum_{i=1} (\\zeta_i+\\zeta_i^*) $$\n",
    "  $$ subject \\quad to \\quad y_i(w^T \\phi(x_i) + b) \\leq \\varepsilon+\\zeta_i $$\n",
    "$$ w^T \\phi(x_i)+b-y \\leq \\varepsilon+\\zeta_i^*  $$\n",
    "$$ \\zeta_i, \\zeta_i^* \\geq 0, \\quad i=1,2,..,n$$\n",
    "\n",
    "şeklinde çözer.\n",
    "\n",
    "Dual problem ise $$min_{ \\alpha, \\alpha^*}(\\alpha-\\alpha^*)^TQ(\\alpha-\\alpha^*)+ \\varepsilon e^T(\\alpha+ \\alpha^*)-y^T(\\alpha -\\alpha^*)$$\n",
    "$$subject\\quad to\\quad e^T(\\alpha-\\alpha^*)=0 $$\n",
    "$$0 \\leq \\alpha_i,\\quad \\alpha_i^* \\leq C,\\quad i=1,2,...,n$$ şeklinde tanımlanır. Burada $\\ e$ bütün elemanları 1 olan matris, $\\ C>0$ üst sınır, $\\ Q $ ise $\\ nxn$ pozitif yarı-tanımlı matris ve $\\ Q_{i,j} \\equiv K=(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)$ kerneldir. Eğitim vektörleri çok boyutlu uzayda $\\ \\phi $ fonksiyonu yardımıyla tanımlanabilirdir.\n",
    "Elde edilen gözlemlere dayanarak istatiksel kurallar uygulanarak $\\ \\sum_{i=1}^n(\\alpha-\\alpha^*)K(x_i,x)+\\rho $ elde edilmiştir.\n",
    "\n",
    "$\\ \\alpha_i-\\alpha_i^* $ farkına dual_coef_, destek vektörleri support_vectors_ ve $\\ \\rho $ bağımsız değişkenine intercept_ parametreleri yoluyla ulaşılabilir.\n",
    "[Kaynak 11](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.114.4288)\n",
    "[Kaynak 12](http://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation)\n",
    "\n",
    "Çok sınıflı sınıflandırma yapılırken SVC ve NuSVC “one-against-one” [Kaynak 13](https://link.springer.com/chapter/10.1007/978-3-642-76153-9_5) yaklaşımını benimsemiştir. Bu yaklaşıma göre $\\ n_class $ sınıf sayısı olmak üzere $\\ \\frac {n_class*(n_class-1)}{2} $ sınıflandırıcı oluşturulur. Bu sınıflandırıcıların her biri iki ayrı sınıftan veriyi eğitirler. \n",
    "\n",
    "LinearSVC ise “one-vs-the-rest” stratejisini benimsemiştir. Bu strateji sınıf başına tek bir sınıflandırıcı eğitimini içerir; bir sınıfa ait veriler pozitif, diğer tüm veriler negatif kabul edilir.[Kaynak 14](http://s3.amazonaws.com/academia.edu.documents/30428242/bg0137.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1494454268&Signature=aPw223N7eRJQ2KV2Hgrj%2BKYveUw%3D&response-content-disposition=inline%3B%20filename%3DPattern_recognition_and_machine_learning.pdf)\n",
    "LinearSVC'nin multi-class ='crammer_singer' parametresi kulanılarak çok sınıflı SVM olarak adlandırılan başka bir strajesi de bulunur. Bu sınıflandırma one-vs-the-rest yaklaşımına göre daha düşük çalışma zamanına sahiptir.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Decision tree\n",
    "\n",
    "Yorumlama için basit ve kullanışlı yöntemlerden olan Ağaç Tabanlı Yöntemler(Tree Based Methods) yüksek doğruluk, kararlılık ve yorumlanma kolaylığı ile tahmini modelleri güçlendirir. Verilen belirli bir gözlem için tahmin yapmak amacıyla genellikle eğitim gözlemlerinin ortalaması ya da ait olduğu bölgedeki modu kullanılır. Tahmini alanı bölmek için kullanılan bölme kuralları kümesi birağaçta özetlenebilir ve bu tip yaklaşımlar karar ağaçları olarak adlandırılır. Karar ağaçlarının otomatik olarak oluşturulması, Morgan & Sonquist (1963) ve Morgan & Messenger (1973) tarafından sosyal bilimler alanında yapılan çalışmalara dayanır.( kaynak1-kitap :W.N. Venables B.D. Ripley Modern Applied Statistics with S-PLUS Third Edition)\n",
    "\n",
    "Karar ağaçları hem regresyon hem de sınıflandırma problemlerine uygulanabilir. Birden fazla bağımsız parametre ve bir bağımlı parametreiçeren data setinde, bağı! mlı parametre kategorik ise sınıflandırma ağacı, sürekli yapıda ise regresyon ağacı adını alır.Yani, her birgözlemin birçok farklı grup veya sınıftan birinden geldiği bilinmektedir ve amaç etiketlenmemiş gözlemleri sınıflandırmak içinöngördürücü değişkenleri kullanmaktır. Bazı durumlarda, analist hangi öngördürücülerin yanıtla ilişkili olduğunu, bunların nasılilişkili olduğunu ve belki de öngörücü değişkenlerin bile başkaları ile tepki tahmininde nasıl etkileşime girdiğini belirlemekisteyebilir. (kaynak kitap chapter 5:High-Dimensional Data Analysis in Cancer Research)\n",
    "\n",
    "Amaç hem kategorik hem nümerik verilerden faydalanarak çıkarılan basit karar kuralları ile bir hedef değişkeninin değerini öngören modeloluşturmaktır.\n",
    "\n",
    "Karar Ağaçları Terminolojisi\n",
    "\n",
    "\n",
    "\n",
    "1. Kök Düğüm: Tüm popülasyonu veya alınmış örneği temsil eder ve iki ya da daha fazla homoje kümeye ayrılır.\n",
    "2. Yaprak Düğüm:Düğümler bölünmüyorsa yaprak düğüm adını alır.\n",
    "3. Karar Düğümü: Bir alt düğüm başka alt düğümlere ayrılırsa, karar düğümü olarak adlandırılır.\n",
    "4. Ata ve Çocuk Düğümü:Belli bir düğümün herhangi bir alt düğümüne çocuk düğüm denir ve verilen düğüm sırasıyla çocuğun üstüdür.\n",
    "5. Alt Ağaç: Ağacın bir alt bölümüne alt ağaç adı verilir.\n",
    "6. Bölme: Bir düğümün iki veya daha fazla alt düğümlere bölünmesi işlemidir.Bunlar karar ağaçları için kullanılan yaygın terimlerdir.\n",
    "Bunlar karar ağaçları için kullanılan yaygın terimlerdir.\n",
    "\n",
    "Sınıflandırma Ağacı\n",
    "Sınıflandırma ağaçları (Classification Trees) 1984 yılında Leo Breiman, Jerome Friedman, Richard Olshen ve Charles Stone tarafından\n",
    "geliştirilen bir yöntemdir. Parametrik olmayan bu yöntem veri madenciliğinde oldukça önemli bir yere sahiptir( kaynak Breiman, L., 2001,\n",
    "Random forests, Machine Learning, 45, 5-32 p).Parametrik olmayan istatistikler ('dağılımsız istatistikler') de denir), bir popülasyonun\n",
    "bazı özelliklerini tanımlayabilen, bu nitelik hakkında hipotezleri test edebilen, diğer bir özellikle olan ilişkisini veya zaman içinde\n",
    "ilgili yapılardaki bu özellikteki farklılıkları test edebilen, popülasyonun veri dağılımı hakkında hiçbir varsayıma ihtiyaç duymayan ve\n",
    "aralık düzeyinde ölçüm gerektirmeyen bir yöntemdir. (kaynak: http://psych.unl.edu/psycrs/971/nonpar/intro.pdf). Kantitatif bir cevap\n",
    "yerine nitel bir cevabı tahmin etmek için kullanılan Sınıflandırma Ağaçlarında sınıf daha önceden atanmış bir değerdir.(kaynak: chapter\n",
    "8.1.2 An Introduction to Statistical Learning with Applications in R ).\n",
    "Bir sınıflandırma ağacı, sıralı bir dizi soruyu sormanın sonucudur ve dizideki her adımda sorulan soru türü, dizinin önceki sorularına verilen\n",
    "cevaplara bağlıdır.Sınıflandırma ağacında eğitim\n",
    "verisinde terminal düğüm tarafından elde edilen değer o bölgedeki gözlemlerin modudur. Böylece, görünmeyen bir veri gözlemi o bölgede\n",
    "düşerse, tahminini mod değeri ile yapacağız. Tahmini alanı belirgin ve çakışmayan bölümlere ayırır. Basit olması adına, bu bölgeleri\n",
    "yüksek boyutlu kutular olarak düşünebiliriz.Stratejik bölünmeler yapma kararı bir ağacın doğruluğunu büyük ölçüde etkiler. Bölünme,\n",
    "kullanıcı tarafından algoritmada tanımlanan bir durdurma kriterine ulaşılana kadar sürer.Bu bölme işlemi durdurma kriterlerine ulaşılana\n",
    "kadar tamamen yetişmiş ağaçlarda sonlanır.\n",
    "\n",
    "Bir ağaç, ilk önce tüm gözlemleri içeren bir 'kök' düğümü düşünülerek yetiştirilir, bu düğümdeki gözlemler tek bir öngörücü değişkende\n",
     "bir 'bölme' kullanılarak bir sol ve bir sağdaki iki alt nesne düğümünden birine gönderilir( kaynak High-Dimensional Data Analysis in\n",
     "Cancer Research chapter 5).Bir düğüm, değişken kümesinin alt kümesidir ve bir terminal olabilir veya terminal olmayabilir. Bir\n",
     "nonterminal (veya ana) düğüm, iki kız düğümüne bölünen bir düğümdür(ikili bölme). Bölünmeyen bir düğüme bir terminal düğümü denir\n",
     "ve bir sınıf etiketi atanır. Kategorik bir öngördürücü değişkeni için, bir bölme sola bir alt grup gönderir ve geri kalanını ise sağa\n",
     "gönderir.Bir ağacın bir düğümü iki soyuna bölmek için kullandığı belirli bölme, her tahmini değişkende olası tüm bölünmeleri göz önüne\n",
     "alarak seçilir. Bazı ölçütlere göre 'en iyi' değeri veren tahmin edici ve bölünmüş kombinasyon, düğümü bölmek için kullanılır. Aynı\n",
     "prosedür, bazen 'özyinelemeli bölümleme' olarak adlandırılan soyundan gelen düğümlere uygulanır.\n",
     "Kategorik tahminci değişkenlerin kullanıldığı sınıflandırma ağaçları için bölünme kuralı, C kategori altkümesine bağlıdır.\n", 
     "(xi ∈ C) olan gözlemler sol alt düğüme atanırken (+ $\\ x_i $ ∉ C ) koşuluna uyan gözlemler ise sağ alt düğüme yerleştirilir. Ağaç oluştururken\n",
     "düğümleri bölerek alt düğümler oluşturmada en çok kullanılan kurallar aşağıdaki gibidir:\n",
     "\n",
    "Sınıflandırma ağacı girdi olarak iki dizi alır: alıştırma örnekleri tutan seyrek veya yoğun bir dizi ve boyutu [n_samples, n_features] \n",
    "olan X dizisi, alıştırma örnekleri için sınıf etiketlerini tutan ve tamsayı değerler alan Y boyut [n_samples] dizisi.\n",
    "\n",
    "Veriyi genelleştirmeyen aşırı karmaşık ağaçlar oluşması, verideki küçük bir değişikliğin tamamen farklı bir yapı oluşturması,\n",
    "öğrenmesi zor kavramlar içermesi gibi nedenler karar ağaçlarının dezavantajlarındandır.\n",
    "En ayırt edici veriyi belirlemek için bilgi kazancı ölçülür. Bilgi kazancı ölçümünde entropi kullanılır.\n",
    "\n",
    "Satır sınıflandırmak için gerekli bilgi :  $$I(s_1,s_2,...,s_m)=-\\sum_i  (si/s)log_2(si/s),   i=1,2,...,m $$\n",
    "\n",
    "\n",
    "\n",
    "Entropi=E(A): $$\\sum_i(s_{1j}+...+s_{mj})/s]I(s_{1j},...,s_{mj})$$\n",
    "\n",
    "\n",
    "\n",
    "Kazanç(A)=$$I(s_1,...,s_m)-E(A)$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[Kaynak 5](www.ayhandemiriz.com/SakaryaWebSite/slides/DMHafta5.ppt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### The data\n",
    "\n",
    "Cam Tanılama Veritabanı\n",
    "Kaynaklar: https://archive.ics.uci.edu/ml/datasets/Glass+Identification\n",
    "\n",
    "Yaratıcı:\tB. German\n",
    "        -- Central Research Establishment\n",
    "           Home Office Forensic Science Service\n",
    "           Aldermaston, Reading, Berkshire RG7 4PN\n",
    "\n",
    "Donör: Vina Spiehler, Ph.D., DABFT\n",
    "               Diagnostic Products Corporation\n",
    "               (213) 776-0180 (ext 3014)\n",
    "\n",
    "Tarih: Eylül 1987\n",
    "\n",
    "Cam çeşitlerinin sınıflandırılması üzerine yapılan çalışma, suç mahallinde kalan camların doğru olarak tanımlanması ve kanıt olarak kullanılabilmesi amacıyla yapılmıştır.\n",
    "Instance sayısı: 214\n",
    "Attribute sayısı:10 \n",
    "Bu attributelara bakacak olursak:\n",
    "\n",
    "1.\tId numarası:1-214 arasında unique bir integer değeri alır.\n",
    "2.\tRI(refractive index): Kırılma endeksi, n malzemeden oluşmuş bir materyal için ışığın maddeden geçişindeki kırılma oranını gösterir. Float bir değer alır.\n",
    "3.\tNa: Bir birim ağırlığa karşılık gelen oksitin içindeki Sodyum’un ağırlık yüzdesidir. Float bir değer alır.\n",
    "4.\tMg: Bir birim ağırlığa karşılık gelen oksitin içindeki Magnezyum’un ağırlık yüzdesidir. Float bir değer alır.\n",
    "5.\tAl: Bir birim ağırlığa karşılık gelen oksitin içindeki Aluminyum’un ağırlık yüzdesidir. Float bir değer alır.\n",
    "6.\tSi: Bir birim ağırlığa karşılık gelen oksitin içindeki Silikon’un ağırlık yüzdesidir. Float bir değer alır.\n",
    "7.\tK: Bir birim ağırlığa karşılık gelen oksitin içindeki Potasyum’un ağırlık yüzdesidir. Float bir değer alır.\n",
    "8.\tCa: Bir birim ağırlığa karşılık gelen oksitin içindeki Kalsiyum’un ağırlık yüzdesidir. Float bir değer alır.\n",
    "9.\tBa:Bir birim ağırlığa karşılık gelen oksitin içindeki Baryum’un ağırlık yüzdesidir. Float bir değer alır.\n",
    "10.\tFe:Bir birim ağırlığa karşılık gelen oksitin içindeki Demir’in ağırlık yüzdesidir. Float bir değer alır.\n",
    "11.\tCam türü (sınıf attribute)\t\n",
    "    1.building_window_float_processed: Binalarda kullanılan eritilmiş camın yüzdürülerek işlem görmesinden oluşan modern camdır.    \n",
    "    2.building_windows_non_float_processed: Binalarda kullanılan yüzdürme işleme görmeden oluşturulan camdır.\n",
    "\t\n",
    "    3.vehicle_windows_float_processed: Araçlarda kullanılan eritilmiş camın yüzdürülerek işlem görmesinden oluşan modern camdır.\n",
    "\t\n",
    "    4.containers: konteyner\n",
    "\t\n",
    "    5.tableware:Sofra takımlarında kullanılan camdır.\n",
    "\t\n",
    "    6.headlamps:Araçlarda kullanılan far camıdır.\n",
    "\n",
    "\n",
    "\n",
    "## Code\n",
    "\n",
    "As proof of work, you must run this notebook.  Upload an HTML output of this notebook on your github account.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "clf.predict([[2., 2.]])\n",
    "array([1])\n",
    "#( http://scikit-learn.org/stable/modules/tree.html )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "glass=pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Veri kumesi icinde belli kolonlari ve satirlari nasil secilecegini ogrenin. Ihtiyaciniz olacak. Asagidaki ornege bakin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = glass.iloc[:, 1:10].values\n",
    "y = glass.iloc[:, 10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.52101  13.64      4.49    ...,   8.75      0.        0.     ]\n",
      " [  1.51761  13.89      3.6     ...,   7.83      0.        0.     ]\n",
      " [  1.51618  13.53      3.55    ...,   7.78      0.        0.     ]\n",
      " ..., \n",
      " [  1.52065  14.36      0.      ...,   8.44      1.64      0.     ]\n",
      " [  1.51651  14.38      0.      ...,   8.48      1.57      0.     ]\n",
      " [  1.51711  14.23      0.      ...,   8.62      1.67      0.     ]] [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n"
     ]
    }
   ],
   "source": [
    "print(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Scikit-Learn kutuphanesi icinde svm ve logistic regression nasil yapiliyor onu ogrenin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier=SVC(kernel='linear', random_state=0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
